{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "HhoE0veRfHoT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telecombcn-dl/2018-dlai-team10/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eYhLQOn3YqGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Convolutional Neural Network**\n",
        "The problem we are trying to solve here is to classify grayscale images of handwritten objects (28 pixels by 28 pixels), into 10 categories (apple, banana, fork...). \n",
        "\n",
        "The dataset we will use is extracted from the Kaggle competition: **Quick Draw! Doodle Recognition Challenge ** (https://www.kaggle.com/c/quickdraw-doodle-recognition). This dataset contains "
      ]
    },
    {
      "metadata": {
        "id": "dwTJ68cmcdlr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**1. Notebook Setting**\n",
        "\n",
        "Import Pytorch and Python libraries (Numpy, Matplotlib...)"
      ]
    },
    {
      "metadata": {
        "id": "b_k3w0d1hxzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import codecs\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Training on the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1euniB3GOHmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset Preparation**\n",
        "\n",
        "Download, reduce, reshape and reorganize dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "E1ww9hWadKaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.1 Dataset Download**\n",
        "\n",
        "The dataset is downloaded from the Google APIs and it comes in the form of a set of Numpy arrays."
      ]
    },
    {
      "metadata": {
        "id": "cfmjdhj88dfN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  urls = [\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy',\n",
        "    ]\n",
        "  \n",
        "  class_name = ['key', 'banana', 'ladder', 'tennis_racquet', 'pizza', 'stop_sign', 'wheel', 'fork', 'book', 'apple']\n",
        "   \n",
        "  def createDir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "  def gen_bar_updater(pbar):\n",
        "    def bar_update(count, block_size, total_size):\n",
        "        if pbar.total is None and total_size:\n",
        "            pbar.total = total_size\n",
        "        progress_bytes = count * block_size\n",
        "        pbar.update(progress_bytes - pbar.n)\n",
        "    return bar_update   \n",
        "    \n",
        "  def download_url(url, root, filename):\n",
        "      from six.moves import urllib\n",
        "      root = os.path.expanduser(root)\n",
        "      fpath = os.path.join(root, filename + \".npy\")\n",
        "\n",
        "      createDir(root)\n",
        "\n",
        "      # downloads file\n",
        "      if os.path.isfile(fpath):\n",
        "          a = 1\n",
        "          #print('Using downloaded and verified file: ' + fpath)\n",
        "      else:\n",
        "          try:\n",
        "              print('Downloading ' + url + ' to ' + fpath)\n",
        "              urllib.request.urlretrieve(\n",
        "                  url, fpath,\n",
        "                  reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "              )\n",
        "          except OSError:\n",
        "              if url[:5] == 'https':\n",
        "                  url = url.replace('https:', 'http:')\n",
        "                  print('Failed download. Trying https -> http instead.'\n",
        "                        ' Downloading ' + url + ' to ' + fpath)\n",
        "                  urllib.request.urlretrieve(\n",
        "                      url, fpath,\n",
        "                      reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "                  )\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "  for i in range(0, len(urls)):\n",
        "    download_url(urls[i], \"data\", class_name[i])\n",
        "    \n",
        "    \n",
        "  print(\"Done!\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPNS_XT5nCIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.2 Dataset Reduction, Reshaping and Reorganization**"
      ]
    },
    {
      "metadata": {
        "id": "l-FFOzLejiUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to avoid overwritting\n",
        "!rm -rf data/train\n",
        "!rm -rf data/validation\n",
        "!rm -rf data/test\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7wbAlMHZ1kr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n",
        "step = ['train', 'validation', 'test']\n",
        "\n",
        "dire = r'data/'\n",
        "\n",
        "max_length = 10000 # Maximum number of files (drawings) per class\n",
        "percen=[0.6, 0.3, 0.1] # Percentage of training, validation and testing\n",
        "\n",
        "begin = [0, int(max_length * percen[0]), int(max_length * (percen[0] + percen[1])) ]\n",
        "end = [int(max_length * (percen[0])), int(max_length * (percen[0] + percen[1])) , max_length-10]\n",
        "\n",
        "for c in range(0, len(class_name)):\n",
        "  print('Class ' + str(c+1) + ' out of ' + str(len(class_name)))\n",
        "  filename = dire + str(class_name[c]) + '.npy'\n",
        "  data = np.load(filename)\n",
        "  \n",
        "  for s in range(0, len(step)):\n",
        "    dire_step = str(dire) + str(step[s])\n",
        "    if not os.path.exists(dire_step):\n",
        "      os.makedirs(dire_step)\n",
        "    \n",
        "    for i in range(begin[s], end[s]):\n",
        "      dire_class = str(dire_step) + '/' + str(class_name[c])\n",
        "      if not os.path.exists(dire_class):\n",
        "        os.makedirs(dire_class)\n",
        "      \n",
        "      # Reshape the raw data into 28x28 images\n",
        "      data_sample = data[i,:].reshape((28, 28))\n",
        "      sample_name = class_name[c] + '_' + str(step[s]) + '_' + str(i)\n",
        "      np.save(os.path.join(dire_class, sample_name), data_sample)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HhoE0veRfHoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.3 Data Visualization**\n",
        "\n",
        "Visualization of a random image corresponding to the training set of images of the selected class. "
      ]
    },
    {
      "metadata": {
        "id": "3Hb-DgpT72zX",
        "colab_type": "code",
        "outputId": "d2060392-5f2b-4a0c-a36f-1211fa23c55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "drawing_class = 0  # 0-apple, 1-banana, 2-book, 3-fork, 4-key, 5-ladder, 6-pizza, 7-stop_sign, 8-tennis_racquet, 9-wheel\n",
        "image_number=random.randint(1,max_length*percen[0])\n",
        "dire = r'data/train/' + str(class_name[drawing_class]) + '/' + str(class_name[drawing_class]) + '_' + 'train' + '_' + str(image_number) +'.npy'\n",
        "data = np.load(dire)\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFblJREFUeJzt3X9MVff9x/HXFeTHHTIQhdWp1Rnb\nkqrb7HSikQo6J92M2mRpJcjMugVndFLXdMSodTGrlbo2olkUW7sOsuY2d1nCH60Qf21EAZWkNZCm\nWNcY4hRBmT8GArL7/aMp+SIXeXO9l3OR5+O/+7lvP/d9PfDinHvu5xyXz+fzCQDwQKOcbgAAhgPC\nEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwCAy0H/4+uuv69NPP5XL5dKWLVs0a9asYPYFAGEl\noLA8c+aMLl26JI/Ho4sXL2rLli3yeDzB7g0AwkZAh+FVVVVasmSJJGnatGm6efOm7ty5E9TGACCc\nBBSWLS0tSkxM7Hk8duxYNTc3B60pAAg3QTnBw7U4ADzqAgrL5ORktbS09Dy+du2axo8fH7SmACDc\nBBSWCxYsUHl5uSSpvr5eycnJiouLC2pjABBOAjobPnv2bD399NN68cUX5XK59NprrwW7LwAIKy4u\n/gsAA2MFDwAYEJYAYEBYAoABYQkABoQlABgQlgBgQFgCgAFhCQAGhCUAGBCWAGAQ8G0lgFDo6Ogw\n1V24cME855NPPul3fPTo0erq6uozBvjDniUAGBCWAGBAWAKAAWEJAAaEJQAYEJYAYEBYAoABYQkA\nBoQlABgQlgBgwHJH9HLr1i2/4/Hx8b2eq62tNc/5+OOPm2s3bNhgqvv444/NcxYXF/sd/9WvfqU/\n//nPfcYAf9izBAADwhIADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAxcPp/P53QT\nCB8ZGRl+x0+cONHruZMnTw5RR/6VlZWZa5ctW+Z3nLs7YjDYswQAA8ISAAwISwAwICwBwICwBAAD\nwhIADAhLADAgLAHAgLAEAANuWHafjo4OU11/N+yaP3++Tp8+3WvszJkz5tefOnWqqe573/ueec7B\n3DDs3//+t+m5xx57zDznlStXzLWLFi0y1S1fvtw85927d/2Ojx49Wt3d3b3GqqurTXPevn3b/PrN\nzc3m2vtXFD3IzZs3+4z99re/1R//+MdeY1FRUeY5V61aZa6dOHGiufZRwJ4lABgEtGdZU1OjTZs2\nafr06ZKkJ554Qtu2bQtqYwAQTgI+DJ87d66KioqC2QsAhC0OwwHAIOCw/OKLL7Ru3TqtXr1ap06d\nCmZPABB2ArqeZVNTk2pra5WVlaXGxkbl5uaqoqJiUGfdAGA4Cegzy5SUFD333HOSpMmTJ2vcuHFq\namrSpEmTgtqcE0b6V4eefPJJv+Off/55r+cG89WZUHx16MSJE+Y5+/vqUExMTJ/nzp49a5qTrw7x\n1SGTsrIyvfvuu5K++kG4fv26UlJSgtoYAISTgPYsMzMz9corr+jYsWPq6urSjh07OAQH8EgLKCzj\n4uJ04MCBYPcCAGFrRNyw7LPPPjPXZmVlmeouXbrkd9zn88nlcvUai42NNb9+e3u7udZq/fr15to7\nd+74HX///ff185//vOfxBx98YJ5zMJ/DWbdVSUmJec63337b73hbW5vcbnevsVD8/w/GqFH2T8YS\nExP7jLW0tGjcuHG9xtra2sxzDub993cjuPsdPnzYPOdgltEONb5nCQAGhCUAGBCWAGBAWAKAAWEJ\nAAaEJQAYEJYAYEBYAoABYQkABoQlABgM27s7+rs8VX/mzp1rrv3+979vqjt58mS/z3355Ze9Hg/m\nEmn9XU7sfn//+9/Nc+bl5ZlrIyP7/5EoKyszzxOojRs3muqOHTtmnvMPf/hDv8/df++otWvXmub0\nt9SwP9HR0eba+5fKBqKlpaXX487OTvO/raioMNdu2rTJVPfDH/7QPOf58+f9jickJOg///lPn7Gh\nxJ4lABgQlgBgQFgCgAFhCQAGhCUAGBCWAGBAWAKAAWEJAAaEJQAYDNsblv3yl7801/7tb38z1zY2\nNprq4uLizHM67f6VDw/S3wqakpISrVmzpudxaWnpQ/f1MAazgmnlypUh7GTkun79uqluxowZ5jkn\nT57sd7ympqbPSqDq6mrTnMFYFSWxZwkAJoQlABgQlgBgQFgCgAFhCQAGhCUAGBCWAGBAWAKAAWEJ\nAAaEJQAYhN1yx88//9xU99RTT5nn/PDDD821P/vZz8y1I9mpU6fMtTt37jTX/vrXvzbVrVixwjwn\nnHXixAlzbWZmpt9xn8/XZ9ni1atXTXOmpKSYX/9B2LMEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwI\nSwAwICwBwICwBAADwhIADMJuueM///lPU92zzz5rnvPGjRvm2sTERHMtgIHdunXLXPvNb37T77i/\n5Y4ff/yxac5ly5aZX/9BTHuWDQ0NWrJkSc/tT69cuaI1a9YoOztbmzZtUmdnZ1CaAYBwNWBYtrW1\naefOnUpLS+sZKyoqUnZ2tv7617/q8ccfl9frDWmTAOC0AcMyKipKhw4dUnJycs9YTU2NFi9eLEnK\nyMhQVVVV6DoEgDAQOWBBZKQiI3uXtbe3KyoqSpKUlJSk5ubm0HQHAGFiwLAcSLDPD6WnpzvyugBC\nIz4+3lz7oN9rp3/nAwpLt9utu3fvKiYmRk1NTb0O0R8WZ8OBR8uIOht+v/nz56u8vFySVFFRoYUL\nFwalGQAIVwPuWdbV1Wn37t26fPmyIiMjVV5erj179qigoEAej0cTJkzQypUrh6JXAHDMgGE5Y8YM\nlZSU9Bl/7733QtIQAISjhz7BE2xTpkwJ+pyNjY3mWj6zBIIrNjY2JPPevHkzJPP2h7XhAGBAWAKA\nAWEJAAaEJQAYEJYAYEBYAoABYQkABoQlABgQlgBgQFgCgEHYLXf81re+Zar7+uLDFkePHjXXzpo1\ny1wLYORgzxIADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwCLvljtZljGvX\nrjXPuXfvXnNtfn6+qW7UKP7OABadnZ0hmdftdodk3v7wGw8ABoQlABgQlgBgQFgCgAFhCQAGhCUA\nGBCWAGBAWAKAAWEJAAYun8/nc7qJQJw/f95c+93vftdce+7cOVPdM888Y54TGMlqamrMtfPmzfM7\n7vP55HK5eo01Njaa5pw4caL59R+EPUsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAADwhIADAhL\nADAgLAHAIOxuWGY1c+ZMc+20adPMtUVFRaa6999/3zwnMJIdOXLEXJuUlGR+7tvf/nbAPQWCPUsA\nMDCFZUNDg5YsWaLS0lJJUkFBgZYvX641a9ZozZo1OnnyZCh7BADHDXgY3tbWpp07dyotLa3X+ObN\nm5WRkRGyxgAgnAy4ZxkVFaVDhw4pOTl5KPoBgLBkvp7lvn37lJiYqJycHBUUFKi5uVldXV1KSkrS\ntm3bNHbs2FD3CgCOCehs+IoVK5SQkKDU1FQVFxdr//792r59e7B7e6DBXLN4+vTp5toFCxaY6jgb\nDtj8/ve/N9fu27fP73hLS4vGjRvXa6y5udk05/0XDQ5UQGfD09LSlJqaKknKzMxUQ0NDUJoBgHAV\nUFhu3Lix55LuNTU1g9pzA4DhaMDD8Lq6Ou3evVuXL19WZGSkysvLlZOTo/z8fMXGxsrtdmvXrl1D\n0SsAOGbAsJwxY4ZKSkr6jP/4xz8OSUMAEI6G7XLHwXxou3XrVnPtL37xC1Pdjh07/I5PnTpVX375\nZZ8x4FFjPcnq9XrNcy5fvtz8XLBO3Fix3BEADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQA\nA8ISAAwISwAwMF/8dzjr6uoy13596bmBfOc73/E7XlFRoaVLl/YaKy8vN7/+UC/hAgJVWVlpqktP\nTzfPefbsWb/jP/jBD3Tu3Lk+Y0OJPUsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAADwhIADAhL\nADAYESt4BuP06dOmugULFvgd9/l8fVbhnDhxwvz6ixYtMtcCwfa///3PXPvMM8+Y6hITE81zHjt2\nzO+4y+Xqc4M0blgGAGGIsAQAA8ISAAwISwAwICwBwICwBAADwhIADAhLADAgLAHAgLAEAINIpxsI\nN/PnzzfVrVixwvxcTk6O+fUvXrxoqouOjjbPCVhVVFSYaz/55BNTXX19vXnOBy1hdPpmfuxZAoAB\nYQkABoQlABgQlgBgQFgCgAFhCQAGhCUAGBCWAGBAWAKAAWEJAAbc3TFAjY2NfscnTZrU57kpU6aY\n53377bdNdb/5zW/Mc2Jka29vN9c+/fTT5trZs2eb6rxer3nOcGZaG15YWKja2lrdu3dPeXl5mjlz\npl599VV1d3dr/PjxevPNNxUVFRXqXgHAMQOGZXV1tS5cuCCPx6PW1latWrVKaWlpys7OVlZWlt56\n6y15vV5lZ2cPRb8A4IgBP7OcM2eO9u7dK0mKj49Xe3u7ampqtHjxYklSRkaGqqqqQtslADhswLCM\niIiQ2+2W9NVnD+np6Wpvb+857E5KSlJzc3NouwQAh5mvZ3n06FF5vV4dPnxYS5cu7RkfqeeHJk2a\nZH6uu7s71O0A/YqNjTXX/utf/wphJ8ObKSwrKyt14MABvfPOOxozZozcbrfu3r2rmJgYNTU1KTk5\nOdR9hh3OhmO44Gx4cAx4GH779m0VFhbq4MGDSkhIkPTV1cTLy8slfXVl5YULF4a2SwBw2IB7lh99\n9JFaW1uVn5/fM/bGG29o69at8ng8mjBhglauXBnSJgHAaQOG5QsvvKAXXnihz/h7770XkoYAIByx\ngmcI/O53vzPX7t+/31R3+fJl85xff3yCR0tHR0efsejo6D7jP/3pT81zDuZrgJ999pmp7kEnQ4cT\n1oYDgAFhCQAGhCUAGBCWAGBAWAKAAWEJAAaEJQAYEJYAYEBYAoABYQkABix3HAL//e9/zbUTJ040\n1f3kJz8xz/mXv/zFXDtqlLN/P60/joP5P42Liwu0nSF36dIlc21eXl6fsSNHjmjZsmW9xv7xj3+Y\n5zx79qy5dsaMGebaRwF7lgBgQFgCgAFhCQAGhCUAGBCWAGBAWAKAAWEJAAaEJQAYEJYAYEBYAoDB\ngLfCxcP7xje+Ya71er2muh/96EfmOTs7O821ixYt8ju+fv16/elPf+p5XFlZaZ5zMHcXLCsrM9V9\n+OGH5jlffPFFv+MffPCBVq9e3Wts27Ztpjk/+eQT8+uXlJSYa48cOWKufeyxx/yOnz9/vtfj06dP\nm+ccaUsYB4M9SwAwICwBwICwBAADwhIADAhLADAgLAHAgLAEAAPCEgAMCEsAMOCGZcPU8ePHzbW5\nubnm2mvXrvkd7+zsVFRUVM/j2bNnm+c8c+aMuda62sm60uZBtR0dHYqOju41Zl3t5HK5zK+/dOlS\nc+26devMtf5uWjd69Gh1dXX1GcPDY88SAAwISwAwICwBwICwBAADwhIADAhLADAgLAHAgLAEAAPC\nEgAMCEsAMGC54whw7949c213d7ff8ejoaHV0dPR6bHX16lVzbUxMjKkuISHBPOf169f9jiclJfV5\nrqmpyTTn5MmTza8fFxdnrkX4Mt3dsbCwULW1tbp3757y8vJ0/Phx1dfX9/zAvvTSS/3eFRAAHgUD\nhmV1dbUuXLggj8ej1tZWrVq1SvPmzdPmzZuVkZExFD0CgOMGDMs5c+Zo1qxZkqT4+Hi1t7f3e6gG\nAI+qAU/wREREyO12S5K8Xq/S09MVERGh0tJS5ebm6uWXX9aNGzdC3igAOMl8gufo0aM6ePCgDh8+\nrLq6OiUkJCg1NVXFxcW6evWqtm/fHupeAcAxphM8lZWVOnDggN555x2NGTNGaWlpPc9lZmZqx44d\noeoPQcDZcM6G4+ENeBh++/ZtFRYW6uDBgz0/oBs3blRjY6MkqaamRtOnTw9tlwDgsAH3LD/66CO1\ntrYqPz+/Z+z5559Xfn6+YmNj5Xa7tWvXrpA2CQBO40vpIwCH4RyG4+Gx3BEADNizBAAD9iwBwICw\nBAADwhIADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAADwhIA\nDAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAADwhIADCKdeNHX\nX39dn376qVwul7Zs2aJZs2Y50UZQ1dTUaNOmTZo+fbok6YknntC2bdsc7ipwDQ0NWr9+vdauXauc\nnBxduXJFr776qrq7uzV+/Hi9+eabioqKcrrNQbn/PRUUFKi+vl4JCQmSpJdeekmLFi1ytslBKiws\nVG1tre7du6e8vDzNnDlz2G8nqe/7On78uOPbasjD8syZM7p06ZI8Ho8uXryoLVu2yOPxDHUbITF3\n7lwVFRU53cZDa2tr086dO5WWltYzVlRUpOzsbGVlZemtt96S1+tVdna2g10Ojr/3JEmbN29WRkaG\nQ109nOrqal24cEEej0etra1atWqV0tLShvV2kvy/r3nz5jm+rYb8MLyqqkpLliyRJE2bNk03b97U\nnTt3hroNPEBUVJQOHTqk5OTknrGamhotXrxYkpSRkaGqqiqn2guIv/c03M2ZM0d79+6VJMXHx6u9\nvX3YbyfJ//vq7u52uCsHwrKlpUWJiYk9j8eOHavm5uahbiMkvvjiC61bt06rV6/WqVOnnG4nYJGR\nkYqJiek11t7e3nM4l5SUNOy2mb/3JEmlpaXKzc3Vyy+/rBs3bjjQWeAiIiLkdrslSV6vV+np6cN+\nO0n+31dERITj28qRzyz/P5/P53QLQTFlyhRt2LBBWVlZamxsVG5urioqKobl50UDeVS22YoVK5SQ\nkKDU1FQVFxdr//792r59u9NtDdrRo0fl9Xp1+PBhLV26tGd8uG+n//++6urqHN9WQ75nmZycrJaW\nlp7H165d0/jx44e6jaBLSUnRc889J5fLpcmTJ2vcuHFqampyuq2gcbvdunv3riSpqanpkTicTUtL\nU2pqqiQpMzNTDQ0NDnc0eJWVlTpw4IAOHTqkMWPGPDLb6f73FQ7basjDcsGCBSovL5ck1dfXKzk5\nWXFxcUPdRtCVlZXp3XfflSQ1Nzfr+vXrSklJcbir4Jk/f37PdquoqNDChQsd7ujhbdy4UY2NjZK+\n+kz2628yDBe3b99WYWGhDh482HOW+FHYTv7eVzhsK5fPgX31PXv26Ny5c3K5XHrttdf01FNPDXUL\nQXfnzh298sorunXrlrq6urRhwwY9++yzTrcVkLq6Ou3evVuXL19WZGSkUlJStGfPHhUUFKijo0MT\nJkzQrl27NHr0aKdbNfP3nnJyclRcXKzY2Fi53W7t2rVLSUlJTrdq5vF4tG/fPk2dOrVn7I033tDW\nrVuH7XaS/L+v559/XqWlpY5uK0fCEgCGG1bwAIABYQkABoQlABgQlgBgQFgCgAFhCQAGhCUAGBCW\nAGDwf/GDPBJqDZcGAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa5b0145860>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SM8HHsj3Xw9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **3. Network Definition**"
      ]
    },
    {
      "metadata": {
        "id": "wbwgqtNKU8ZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**3.1 Mini-Batch Definition**"
      ]
    },
    {
      "metadata": {
        "id": "ZTTQPubmvVjj",
        "colab_type": "code",
        "outputId": "2239765f-bcfa-46ad-d140-ed3940301772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def load_sample(x):\n",
        "\treturn np.load(x)\n",
        "\n",
        "\n",
        "bs = 30\n",
        "train_dir = r\"data/train\"\n",
        "val_dir = r\"data/validation\"\n",
        "test_dir = r\"data/test\"\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "valid_dataset = datasets.DatasetFolder(val_dir, extensions = ['.npy'], loader = load_sample)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "valid_iter = iter(valid_loader)\n",
        "\n",
        "test_dataset = datasets.DatasetFolder(test_dir, extensions = ['.npy'], loader = load_sample)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "test_iter = iter(test_loader)\n",
        "\n",
        "batch, labels = train_iter.next()\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Dbh7DIbgvgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.2 CNN Definition and Forward Pass**"
      ]
    },
    {
      "metadata": {
        "id": "6xNq_vmpwdTz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        \n",
        "        #self.bn1 = nn.BatchNorm2d(6)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        #self.bn2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(16, 16, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
        "        \n",
        "        #self.bn3 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(16, 32, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
        "        \n",
        "        #self.bn4 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(32, 32, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv5.weight)\n",
        "        \n",
        "        #self.bn5 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = F.relu(self.bn1(self.conv1(x)))\n",
        "        #x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu((self.conv1(x)))\n",
        "        x = F.relu((self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        #x = F.relu(self.bn3(self.conv3(x)))\n",
        "        #x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu((self.conv3(x)))\n",
        "        x = F.relu((self.conv4(x)))\n",
        "        x = self.pool(x)\n",
        "        #x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu((self.conv5(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(net)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAA_NJoVYejA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.3 Loss Function and Optimizer Definition**\n",
        "\n",
        "As it is a classification, we have chosen to used the Cross Entropy Loss. As the optimizer we will use the Gradient Descent algorithm, having the learning rate and momentum as hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "2_X0YAHg316V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.000001)\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esOsTmQft5fZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **4. Network Training**"
      ]
    },
    {
      "metadata": {
        "id": "lqBWarLoV4Z1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 Training and Validation"
      ]
    },
    {
      "metadata": {
        "id": "eKoLV6eSt85B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To plot the results\n",
        "training_loss_list = []\n",
        "validation_loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    training_accuracy = 0.0\n",
        "    training_total = 0.0\n",
        "    training_correct = 0.0\n",
        "    \n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      \n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        inputs = inputs.view(bs,1,28,28).float()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = outputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1) #gets the index of the maximum predicted value\n",
        "        training_total = training_total + labels.size(0)\n",
        "        training_correct = training_correct + (predicted == labels).sum().item() #accumulate correct\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            training_accuracy=training_correct/training_total\n",
        "            print('[%d, %5d] Training Loss: %.3f - Training Accuracy: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200,training_accuracy))\n",
        "            \n",
        "            \n",
        "            training_loss_list.append(running_loss/200)\n",
        "            running_loss = 0.0\n",
        "            total=0.0\n",
        "            correct=0.0\n",
        "            \n",
        "    with torch.no_grad():\n",
        "      \n",
        "      running_validation_loss=0.0\n",
        "      validation_accuracy=0.0\n",
        "      validation_total=0.0\n",
        "      validation_correct=0.0\n",
        "      \n",
        "      for j, valid_data in enumerate(valid_loader,0):     \n",
        "        valid_inputs, valid_labels = valid_data\n",
        "        valid_inputs = valid_inputs.view(bs, 1, 28, 28).float()\n",
        "        valid_inputs = valid_inputs.to(device)\n",
        "        valid_labels = valid_labels.to(device)\n",
        "        valid_outputs = net(valid_inputs)\n",
        "        valid_loss = criterion(valid_outputs, valid_labels)\n",
        "        running_validation_loss += valid_loss.item()\n",
        "        \n",
        "        _,predicted=torch.max(valid_outputs.data,1)\n",
        "        validation_total=validation_total+valid_labels.size(0)\n",
        "        validation_correct=validation_correct + (predicted == valid_labels).sum().item()\n",
        "        \n",
        "      validation_accuracy=validation_correct/validation_total\n",
        "      print('[%d] Validation Loss: %.3f - Validation Accuracy: %.3f' %\n",
        "          (epoch + 1, running_validation_loss/len(valid_loader), validation_accuracy))\n",
        "      validation_loss_list.append(valid_loss)\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8S-g-OYCVmiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 Results Visualization"
      ]
    },
    {
      "metadata": {
        "id": "ooU4g0RWHAxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_examples = 6e4\n",
        "plot_every = 200 #batches\n",
        "training_loss_np = np.asarray(training_loss_list)\n",
        "validation_loss_np = np.asarray(validation_loss_list)\n",
        "\n",
        "x_axis_train = np.arange(1, len(training_loss_list)+1)\n",
        "x_axis_validation = np.arange(1, len(validation_loss_list)+1)\n",
        "\n",
        "p1=plt.plot(x_axis_train * bs * plot_every / training_examples, training_loss_np)\n",
        "p2=plt.plot(x_axis_validation, validation_loss_np)\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.title('Loss')\n",
        "plt.legend((p1[0], p2[0]), ('Training Loss', 'Validation Loss'))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYGMvzBa1mky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **5. Network Testing**"
      ]
    },
    {
      "metadata": {
        "id": "2bBNj1FWgr58",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Test Accuracy and Loss Computation"
      ]
    },
    {
      "metadata": {
        "id": "GuiC_DNl1q_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "running_test_loss=0.0\n",
        "test_accuracy=0.0\n",
        "test_total=0.0\n",
        "test_correct=0.0\n",
        "  \n",
        "for i, test_data in enumerate(test_loader,0): \n",
        "  test_inputs, test_labels = test_data\n",
        "  test_inputs = test_inputs.view(bs, 1, 28, 28).float()\n",
        "  test_inputs = test_inputs.to(device)\n",
        "  test_labels = test_labels.to(device)\n",
        "  test_outputs = net(test_inputs)\n",
        "  test_loss = criterion(test_outputs, test_labels)\n",
        "  running_test_loss += test_loss.item()\n",
        "  \n",
        "  _,predicted=torch.max(test_outputs.data,1)\n",
        "  test_total=test_total+test_labels.size(0)\n",
        "  test_correct=test_correct + (predicted == test_labels).sum().item()        \n",
        "  test_accuracy[i]=test_correct/test_total\n",
        "  \n",
        "print('Test Loss: %.3f - Test Accuracy: %.3f' %\n",
        "          (running_test_loss/len(test_loader), test_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OaQBGjTvfeoU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Performance Demo"
      ]
    },
    {
      "metadata": {
        "id": "djkk2I7SnB21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model on the test data:\n",
        "\n",
        "While our densely-connected network we had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we decreased our error rate by 68% (relative)."
      ]
    },
    {
      "metadata": {
        "id": "Z_hhvSIRpzvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#WHICH CLASSESS PERFORMED BETTER\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}