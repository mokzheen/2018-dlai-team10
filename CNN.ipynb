{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "HhoE0veRfHoT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eYhLQOn3YqGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Convolutional Neural Network**\n",
        "The problem we are trying to solve here is to classify grayscale images of handwritten objects (28 pixels by 28 pixels), into 10 categories (apple, banana, fork...). \n",
        "\n",
        "The dataset we will use is extracted from the Kaggle competition: **Quick Draw! Doodle Recognition Challenge ** (https://www.kaggle.com/c/quickdraw-doodle-recognition). This dataset contains "
      ]
    },
    {
      "metadata": {
        "id": "dwTJ68cmcdlr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**1. Notebook Setting**\n",
        "\n",
        "Import Pytorch and Python libraries (Numpy, Matplotlib...)"
      ]
    },
    {
      "metadata": {
        "id": "b_k3w0d1hxzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import codecs\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADImzkBIqHd_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Training on the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1euniB3GOHmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset Preparation**\n",
        "\n",
        "Download, reduce, reshape and reorganize dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "E1ww9hWadKaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.1 Download the Dataset:**\n",
        "\n",
        "The dataset is downloaded from the Google APIs and it comes in the form of a set of Numpy arrays."
      ]
    },
    {
      "metadata": {
        "id": "cfmjdhj88dfN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  urls = [\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy',\n",
        "    ]\n",
        "  \n",
        "  class_name = ['key', 'banana', 'ladder', 'tennis_racquet', 'pizza', 'stop_sign', 'wheel', 'fork', 'book', 'apple']\n",
        "   \n",
        "  def createDir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "  def gen_bar_updater(pbar):\n",
        "    def bar_update(count, block_size, total_size):\n",
        "        if pbar.total is None and total_size:\n",
        "            pbar.total = total_size\n",
        "        progress_bytes = count * block_size\n",
        "        pbar.update(progress_bytes - pbar.n)\n",
        "    return bar_update   \n",
        "    \n",
        "  def download_url(url, root, filename):\n",
        "      from six.moves import urllib\n",
        "      root = os.path.expanduser(root)\n",
        "      fpath = os.path.join(root, filename + \".npy\")\n",
        "\n",
        "      createDir(root)\n",
        "\n",
        "      # downloads file\n",
        "      if os.path.isfile(fpath):\n",
        "          a = 1\n",
        "          #print('Using downloaded and verified file: ' + fpath)\n",
        "      else:\n",
        "          try:\n",
        "              print('Downloading ' + url + ' to ' + fpath)\n",
        "              urllib.request.urlretrieve(\n",
        "                  url, fpath,\n",
        "                  reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "              )\n",
        "          except OSError:\n",
        "              if url[:5] == 'https':\n",
        "                  url = url.replace('https:', 'http:')\n",
        "                  print('Failed download. Trying https -> http instead.'\n",
        "                        ' Downloading ' + url + ' to ' + fpath)\n",
        "                  urllib.request.urlretrieve(\n",
        "                      url, fpath,\n",
        "                      reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "                  )\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "  for i in range(0, len(urls)):\n",
        "    download_url(urls[i], \"data\", class_name[i])\n",
        "    \n",
        "    \n",
        "  print(\"Done!\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPNS_XT5nCIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.2 Reduction, reshape and reorganization of the Dataset:**"
      ]
    },
    {
      "metadata": {
        "id": "C7wbAlMHZ1kr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n",
        "step = ['train', 'validation', 'test']\n",
        "\n",
        "dire = r'data/'\n",
        "\n",
        "max_length = 10000 # Maximum number of files (drawings) per class\n",
        "percen=[0.6, 0.3, 0.1] # Percentage of training, validation and testing\n",
        "\n",
        "begin = [0, int(max_length * percen[0]), int(max_length * (percen[0] + percen[1])) + 1]\n",
        "end = [int(max_length * (percen[0])), int(max_length * (percen[0] + percen[1])) + 1, max_length]\n",
        "\n",
        "for c in range(0, len(class_name)):\n",
        "  print('Class ' + str(c+1) + ' out of ' + str(len(class_name)))\n",
        "  filename = dire + str(class_name[c]) + '.npy'\n",
        "  data = np.load(filename)\n",
        "  \n",
        "  for s in range(0, len(step)):\n",
        "    dire_step = str(dire) + str(step[s])\n",
        "    if not os.path.exists(dire_step):\n",
        "      os.makedirs(dire_step)\n",
        "    \n",
        "    for i in range(begin[s], end[s]):\n",
        "      dire_class = str(dire_step) + '/' + str(class_name[c])\n",
        "      if not os.path.exists(dire_class):\n",
        "        os.makedirs(dire_class)\n",
        "      \n",
        "      # Reshape the raw data into 28x28 images\n",
        "      data_sample = data[i,:].reshape((28, 28))\n",
        "      sample_name = class_name[c] + '_' + str(step[s]) + '_' + str(i)\n",
        "      np.save(os.path.join(dire_class, sample_name), data_sample)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HhoE0veRfHoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.3 Dataset Visualization:**\n",
        "\n",
        "Visualization of a random image corresponding to the training set of images of the selected class. "
      ]
    },
    {
      "metadata": {
        "id": "jh0RVxYHnZbS",
        "colab_type": "code",
        "outputId": "51a73079-fae0-4bde-a258-6763dfe07287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "drawing_class = 8  # 0-apple, 1-banana, 2-book, 3-fork, 4-key, 5-ladder, 6-pizza, 7-stop_sign, 8-tennis_racquet, 9-wheel\n",
        "image_number=random.randint(1,max_length*percen[0])\n",
        "dire = r'data/train/' + str(class_name[drawing_class]) + '/' + str(class_name[drawing_class]) + '_' + 'train' + '_' + str(image_number) +'.npy'\n",
        "data = np.load(dire)\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe3630d4f98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SM8HHsj3Xw9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **3. Network Definition**"
      ]
    },
    {
      "metadata": {
        "id": "wbwgqtNKU8ZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**3.1 Mini-batch definition**"
      ]
    },
    {
      "metadata": {
        "id": "ZTTQPubmvVjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_sample(x):\n",
        "\treturn np.load(x)\n",
        "\n",
        "\n",
        "bs = 32\n",
        "train_dir = r\"data/train\"\n",
        "val_dir = r\"data/validation\"\n",
        "test_dir = r\"data/test\"\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "valid_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n",
        "valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "valid_iter = iter(train_loader)\n",
        "\n",
        "test_dataset = datasets.DatasetFolder(test_dir, extensions = ['.npy'], loader = load_sample)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "test_iter = iter(test_loader)\n",
        "\n",
        "batch, labels = train_iter.next()\n",
        "print(batch.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Dbh7DIbgvgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.2 CNN definition and Forward Pass**"
      ]
    },
    {
      "metadata": {
        "id": "6xNq_vmpwdTz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(16, 16, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(16, 32, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(32, 32, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv5.weight)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAA_NJoVYejA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.3 Loss Function and Optimizer Definition**\n",
        "\n",
        "As it is a classification, we have chosen to used the Cross Entropy Loss. As the optimizer we will use the Gradient Descent algorithm, having the learning rate and momentum as hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "2_X0YAHg316V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esOsTmQft5fZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **4. Network Training**"
      ]
    },
    {
      "metadata": {
        "id": "eKoLV6eSt85B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To plot the results\n",
        "training_loss_list = []\n",
        "validation_loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        inputs = inputs.view(bs,1,28,28).float()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = outputs.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] Training loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            training_loss_list.append(running_loss/200)\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "              valid_inputs, valid_labels = valid_iter.next()\n",
        "              valid_inputs = valid_inputs.view(bs, 1, 28, 28).float()\n",
        "              valid_inputs = valid_inputs.to(device)\n",
        "              valid_labels = valid_labels.to(device)\n",
        "              valid_outputs = net(valid_inputs)\n",
        "              valid_loss = criterion(valid_outputs, valid_labels)\n",
        "              print('[%d, %5d] Validation loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, valid_loss))\n",
        "              validation_loss_list.append(valid_loss)\n",
        "            \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooU4g0RWHAxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_examples = 6e5\n",
        "plot_every = 200 #batches\n",
        "training_loss_np = np.asarray(training_loss_list)\n",
        "validation_loss_np = np.asarray(validation_loss_list)\n",
        "\n",
        "x_axis = np.arange(0, len(training_loss_list))\n",
        "plt.plot(x_axis * bs * plot_every / training_examples, training_loss_np)\n",
        "plt.plot(x_axis * bs * plot_every / training_examples, validation_loss_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYGMvzBa1mky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **5. Network Testing**"
      ]
    },
    {
      "metadata": {
        "id": "OwiNwLg-d_Nz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuiC_DNl1q_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print('GroundTruth: ', ' '.join('%5s' % class_name[labels[j]] for j in range(4)))\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % class_name[predicted[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "djkk2I7SnB21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model on the test data:\n",
        "\n",
        "While our densely-connected network we had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we decreased our error rate by 68% (relative)."
      ]
    },
    {
      "metadata": {
        "id": "PXPvUyOZpj8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ACCURACY OF THE NETWORK\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_hhvSIRpzvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#WHICH CLASSESS PERFORMED BETTER\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RBL7aifncYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Coses que falten:**\n",
        "\n",
        "1.   Fer proves hiperparametres/tamany xarxa/arquitectura per millorar el train\n",
        "2.   Afegir plots de les losses, accuracy\n",
        "3.   Afegir final st\n",
        "4.   Redactar tots els apartats\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}