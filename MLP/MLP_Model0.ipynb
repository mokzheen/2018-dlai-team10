{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP Model0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "i0hJACKxoOa3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "\n",
        "# Constants\n",
        "IMAGE_WIDTH = 28\n",
        "IMAGE_HEIGHT = 28\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "TRAIN_FOLDER = r\"data/train\"\n",
        "VALID_FOLDER = r\"data/validation\"\n",
        "TEST_FOLDER = r\"data/test\"\n",
        "RESULT_FOLDER = r\"results\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "doYZZAALUmSt",
        "colab_type": "code",
        "outputId": "f4ef76e7-e485-47cb-9c6d-591e710b78c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "cell_type": "code",
      "source": [
        "  #Init folder data (NPYs)\n",
        "  \n",
        "  urls = [\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy',\n",
        "    ]\n",
        "  \n",
        "  class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n",
        "   \n",
        "  def createDir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "  def gen_bar_updater(pbar):\n",
        "    def bar_update(count, block_size, total_size):\n",
        "        if pbar.total is None and total_size:\n",
        "            pbar.total = total_size\n",
        "        progress_bytes = count * block_size\n",
        "        pbar.update(progress_bytes - pbar.n)\n",
        "    return bar_update   \n",
        "    \n",
        "  def download_url(url, root, filename):\n",
        "      from six.moves import urllib\n",
        "      root = os.path.expanduser(root)\n",
        "      fpath = os.path.join(root, filename + \".npy\")\n",
        "\n",
        "      createDir(root)\n",
        "\n",
        "      # downloads file\n",
        "      if os.path.isfile(fpath):\n",
        "          a = 1\n",
        "          #print('Using downloaded and verified file: ' + fpath)\n",
        "      else:\n",
        "          try:\n",
        "              print('Downloading ' + url + ' to ' + fpath)\n",
        "              urllib.request.urlretrieve(\n",
        "                  url, fpath,\n",
        "                  reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "              )\n",
        "          except OSError:\n",
        "              if url[:5] == 'https':\n",
        "                  url = url.replace('https:', 'http:')\n",
        "                  print('Failed download. Trying https -> http instead.'\n",
        "                        ' Downloading ' + url + ' to ' + fpath)\n",
        "                  urllib.request.urlretrieve(\n",
        "                      url, fpath,\n",
        "                      reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "                  )\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "  for i in range(0, len(urls)):\n",
        "    download_url(urls[i], \"data\", class_name[i])\n",
        "\n",
        "  print(\"----> Download Done!\")\n",
        "    \n",
        "  step = ['train', 'validation', 'test']\n",
        "\n",
        "  dire = r'data/'\n",
        "  createDir(RESULT_FOLDER)\n",
        "\n",
        "  max_length = 100000 # Maximum number of files (drawings) per class\n",
        "  percen=[0.6, 0.3, 0.1] # Percentage of training, validation and testing\n",
        "\n",
        "  begin = [0, int(max_length * percen[0]), int(max_length * (percen[0] + percen[1])) + 1]\n",
        "  end = [int(max_length * (percen[0])), int(max_length * (percen[0] + percen[1])) + 1, max_length]\n",
        "\n",
        "  for c in range(0, len(class_name)):\n",
        "    print('Class ' + str(c+1) + ' out of ' + str(len(class_name)))\n",
        "    filename = dire + str(class_name[c]) + '.npy'\n",
        "    data = np.load(filename)\n",
        "\n",
        "    for s in range(0, len(step)):\n",
        "      dire_step = str(dire) + str(step[s])\n",
        "      if not os.path.exists(dire_step):\n",
        "        os.makedirs(dire_step)\n",
        "\n",
        "      for i in range(begin[s], end[s]):\n",
        "        dire_class = str(dire_step) + '/' + str(class_name[c])\n",
        "        if not os.path.exists(dire_class):\n",
        "          os.makedirs(dire_class)\n",
        "\n",
        "        # Reshape the raw data into 28x28 images\n",
        "        data_sample = data[i,:].reshape((28, 28))\n",
        "        sample_name = class_name[c] + '_' + str(step[s]) + '_' + str(i)\n",
        "        np.save(os.path.join(dire_class, sample_name), data_sample)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0.00/126M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy to data/apple.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "126MB [00:01, 122MB/s]                           \n",
            "  0%|          | 0.00/241M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy to data/banana.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "241MB [00:01, 137MB/s]                           \n",
            "  1%|▏         | 1.47M/98.3M [00:00<00:06, 14.7MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy to data/book.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "98.3MB [00:00, 109MB/s]                             \n",
            "  1%|          | 1.13M/181M [00:00<00:15, 11.3MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy to data/fork.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "181MB [00:01, 109MB/s]                            \n",
            "  1%|          | 827k/102M [00:00<00:12, 8.27MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy to data/key.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "102MB [00:01, 71.5MB/s]                            \n",
            "  2%|▏         | 1.52M/93.9M [00:00<00:06, 15.2MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy to data/ladder.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "93.9MB [00:00, 113MB/s]                             \n",
            "  0%|          | 0.00/107M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy to data/pizza.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "107MB [00:01, 79.5MB/s]                            \n",
            "  2%|▏         | 2.01M/98.8M [00:00<00:04, 20.1MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy to data/stop_sign.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "98.8MB [00:01, 88.5MB/s]                            \n",
            "  1%|          | 975k/93.6M [00:00<00:09, 9.74MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy to data/tennis_racquet.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "93.6MB [00:00, 104MB/s]                             \n",
            "  1%|          | 1.34M/113M [00:00<00:08, 13.3MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy to data/wheel.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "113MB [00:01, 107MB/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----> Download Done!\n",
            "Class 1 out of 10\n",
            "Class 2 out of 10\n",
            "Class 3 out of 10\n",
            "Class 4 out of 10\n",
            "Class 5 out of 10\n",
            "Class 6 out of 10\n",
            "Class 7 out of 10\n",
            "Class 8 out of 10\n",
            "Class 9 out of 10\n",
            "Class 10 out of 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q_21--AuVd_2",
        "colab_type": "code",
        "outputId": "c5756ece-1049-4c34-be6d-6c9c3839649d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls data/test\n",
        "!ls data/train\n",
        "!ls data/validation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apple  banana  book  fork  key\tladder\tpizza  stop_sign  tennis_racquet  wheel\n",
            "apple  banana  book  fork  key\tladder\tpizza  stop_sign  tennis_racquet  wheel\n",
            "apple  banana  book  fork  key\tladder\tpizza  stop_sign  tennis_racquet  wheel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BEytNHW0Uk9v",
        "colab_type": "code",
        "outputId": "e8972f19-971e-4fa6-ca46-f9ba8a670a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001        #.00001, 0.0001, 0.001, 0.1, 0.01\n",
        "EPOCHS = 40\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(IMAGE_WIDTH * IMAGE_HEIGHT, 500)\n",
        "        self.fc2 = nn.Linear(500, 256)\n",
        "        self.fc3 = nn.Linear(256, N_CLASSES)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, IMAGE_WIDTH * IMAGE_HEIGHT)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_sample(x):\n",
        "\treturn np.load(x)\n",
        "\n",
        "def train(epoch, net, train_loader, opt):\n",
        "    net.train()\n",
        "    correct = 0\n",
        "    for j, item in enumerate(train_loader, 0):\n",
        "      inputs, labels = item\n",
        "\n",
        "      inputs = inputs.view(BATCH_SIZE, 1, IMAGE_WIDTH, IMAGE_HEIGHT).float()\n",
        "      if CUDA:\n",
        "          inputs = inputs.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "      # Reset gradients\n",
        "      opt.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      pred = outputs.data.max(1)[1]   # get the index of the max log-probability\n",
        "      correct += pred.eq(labels.data).cpu().sum()\n",
        "      accuracy = 100. * correct / len(train_loader.dataset)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()                 # calculate the gradients (backpropagation)\n",
        "      optimizer.step()                # update the weights\n",
        "\n",
        "      if j % 200 == 199:\n",
        "        txt = '[%d, %5d] loss: %.3f' % (epoch, j + 1, loss.item())\n",
        "        print(txt)\n",
        "\n",
        "      \n",
        "\n",
        "def validate(net, val_loader):\n",
        "    net.eval()\n",
        "    val_loss, correct, j = 0, 0, 0\n",
        "    for inputs, labels in val_loader:\n",
        "\n",
        "        inputs = inputs.view(BATCH_SIZE, 1, IMAGE_WIDTH, IMAGE_HEIGHT).float()\n",
        "        if CUDA:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        val_loss += criterion(outputs, labels)\n",
        "        pred = outputs.data.max(1)[1]\n",
        "        correct += pred.eq(labels.data).cpu().sum()\n",
        "\n",
        "        if j % 200 == 199:\n",
        "          txt = '[%d, %5d] validation: %.3f' % (epoch, j + 1, loss.item())\n",
        "          print(txt)\n",
        "        j = j + 1\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100. * correct / len(val_loader.dataset)\n",
        "\n",
        "    print('\\n Validation set  --  Average loss: {:.4f} \\t Accuracy: {}/{} ({:.0f}%) \\n'.format(\n",
        "        val_loss, correct, len(val_loader.dataset), accuracy))\n",
        "              \n",
        "    return val_loss, accuracy\n",
        "  \n",
        "def plot(acc_vector, loss_vector):\n",
        "    epochs = [i for i in range(1, EPOCHS + 1)]\n",
        "    plt.plot(epochs, acc_vector)\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs, loss_vector)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "#  --  Start Training    \n",
        "    \n",
        "name = \"MLP_model0\" + \"_Adam_\" + str(BATCH_SIZE) + \"_\" + str(LEARNING_RATE).replace(\".\", \"c\") + \"_\" + str(EPOCHS)\n",
        "#print(name)\n",
        "\n",
        "train_dataset = datasets.DatasetFolder(TRAIN_FOLDER, extensions = ['.npy'], loader = load_sample)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n",
        "\n",
        "test_dataset = datasets.DatasetFolder(TEST_FOLDER, extensions = ['.npy'], loader = load_sample)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)\n",
        "\n",
        "val_dataset = datasets.DatasetFolder(VALID_FOLDER, extensions = ['.npy'], loader = load_sample)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)\n",
        "\n",
        "\n",
        "\n",
        "net = MLP()\n",
        "\n",
        "  \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
        "if CUDA:\n",
        "    net.cuda()\n",
        "\n",
        "loss_vector = []\n",
        "acc_vector = []\n",
        "for ep in range(EPOCHS):  # epochs loop\n",
        "\n",
        "    #train\n",
        "    loss_info = train(ep, net, train_loader, optimizer)\n",
        "    \n",
        "    #validate\n",
        "    #val_loss, accuracy = validate(net, val_loader)\n",
        "    \n",
        "\n",
        "    #loss_vector.append(val_loss)\n",
        "    #acc_vector.append(accuracy)\n",
        "\n",
        "    \n",
        "# Plot train loss and validation accuracy vs epochs for each learning rate\n",
        "#plot(acc_vector, loss_vector)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0,   200] loss: 1.092\n",
            "[0,   400] loss: 0.862\n",
            "[0,   600] loss: 0.619\n",
            "[0,   800] loss: 0.915\n",
            "[0,  1000] loss: 0.738\n",
            "[0,  1200] loss: 0.648\n",
            "[0,  1400] loss: 0.490\n",
            "[0,  1600] loss: 0.315\n",
            "[0,  1800] loss: 0.831\n",
            "[0,  2000] loss: 0.327\n",
            "[0,  2200] loss: 0.682\n",
            "[0,  2400] loss: 0.351\n",
            "[0,  2600] loss: 0.419\n",
            "[0,  2800] loss: 0.483\n",
            "[0,  3000] loss: 0.498\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}