{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","version":"0.3.2","provenance":[{"file_id":"13WVLdUAVQNXdb05S72S0WsQtZPxvNURj","timestamp":1544464942569},{"file_id":"https://github.com/telecombcn-dl/2018-dlai-team10/blob/master/CNN.ipynb","timestamp":1544262715538}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"eYhLQOn3YqGA","colab_type":"text"},"cell_type":"markdown","source":["#**LSTM**\n","The problem we are trying to solve here is to classify variable sized sequences of drawings into 10 categories (apple, banana, fork...). \n","\n","We are tackling this problem with an LSTM. The input of the LSTM are the keypoints of the drawings in the order in which they were drawn.\n","\n","The dataset we will use is extracted from the Kaggle competition: **Quick Draw! Doodle Recognition Challenge ** (https://www.kaggle.com/c/quickdraw-doodle-recognition). This dataset contains "]},{"metadata":{"id":"dwTJ68cmcdlr","colab_type":"text"},"cell_type":"markdown","source":["#**1. Notebook Setting**\n","\n","Import Pytorch and Python libraries (Numpy, Matplotlib...)"]},{"metadata":{"id":"b_k3w0d1hxzB","colab_type":"code","outputId":"31c0b2b9-4360-41be-d469-efe6f470db98","executionInfo":{"status":"ok","timestamp":1544912461055,"user_tz":-60,"elapsed":36487,"user":{"displayName":"Ponç Palau Puigdevall","photoUrl":"https://lh4.googleusercontent.com/-y5VtTnBIWvE/AAAAAAAAAAI/AAAAAAAAApg/KjCyq610BUQ/s64/photo.jpg","userId":"14934553051290892540"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","  \n","import numpy as np\n","import os\n","import sys\n","import torch\n","import torchvision\n","import random\n","import codecs\n","import torch.utils.data\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from tqdm import tqdm\n","from torchvision import datasets, transforms\n","\n","print('Done!')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tcmalloc: large alloc 1073750016 bytes == 0x57884000 @  0x7f48c55b82a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"ADImzkBIqHd_","colab_type":"code","outputId":"a4d93d98-0485-480a-a0ae-2cc9be2c47dc","executionInfo":{"status":"ok","timestamp":1544912463377,"user_tz":-60,"elapsed":38788,"user":{"displayName":"Ponç Palau Puigdevall","photoUrl":"https://lh4.googleusercontent.com/-y5VtTnBIWvE/AAAAAAAAAAI/AAAAAAAAApg/KjCyq610BUQ/s64/photo.jpg","userId":"14934553051290892540"}},"colab":{"base_uri":"https://localhost:8080/","height":345}},"cell_type":"code","source":["#Training on the GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda:0\n","Sat Dec 15 22:21:01 2018       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P8    26W / 149W |     11MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"metadata":{"id":"1euniB3GOHmk","colab_type":"text"},"cell_type":"markdown","source":["# **2. Dataset Preparation**\n","\n","Download, reduce, reshape and reorganize dataset\n"]},{"metadata":{"id":"E1ww9hWadKaR","colab_type":"text"},"cell_type":"markdown","source":["## **2.1 Download the Dataset:**\n","\n","The dataset is downloaded from Google Drive and it comes in the form of a csv files with the sequences. The link is public."]},{"metadata":{"id":"cfmjdhj88dfN","colab_type":"code","outputId":"56571783-6405-4dbb-89af-ec19d338ec1a","executionInfo":{"status":"ok","timestamp":1544912505999,"user_tz":-60,"elapsed":81355,"user":{"displayName":"Ponç Palau Puigdevall","photoUrl":"https://lh4.googleusercontent.com/-y5VtTnBIWvE/AAAAAAAAAAI/AAAAAAAAApg/KjCyq610BUQ/s64/photo.jpg","userId":"14934553051290892540"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"cell_type":"code","source":["  !pip install googledrivedownloader\n","  \n","  from google_drive_downloader import GoogleDriveDownloader as gdd\n","  urls = [\n","        '1JmW1jzvcDCSRg2vr6nBfmnWyR2K665Ny', \n","        '1_DRuC1dnG6Rsfb-a32eXD0fh_dpyiuJU',\n","        '1cBXEmlIAwuOmwPEgd9kbnSvjofGEFIAU',\n","        '1OZdyx5rXytzXnvTq8S3LZS_Ewl_iyIio',\n","        '1585P-SU8G_vNGu78yEJhOpH9-ajxz3fm',\n","        '1a9KvLtNi3crhi3iqBX93UwKlmPlmxnyi',\n","        '1lnneEBuc2K4papzkui14ZiYN58dMM8VZ',\n","        '1KdDex8cjZc-SNR8NVURsK-OaXtfxEskU',\n","        '1fODQI_9LtXyXVk9RNGC7VWZxUlsahxKk',\n","        '1hbRfukgoLJoQeGtjL820cR89hEqBkzk4'\n","    ]\n","  \n","  class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n","\n","    \n","  def createDir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","  extension = '.csv'\n","                  \n","  for i in range(0, len(urls)):\n","    createDir('data')\n","    name = class_name[i] + extension\n","    gdd.download_file_from_google_drive(file_id = urls[i],\n","                                       dest_path = os.path.join('data', name))\n","    \n","    \n","  print(\"Done!\")   "],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.3)\n","Downloading 1JmW1jzvcDCSRg2vr6nBfmnWyR2K665Ny into data/apple.csv... Done.\n","Downloading 1_DRuC1dnG6Rsfb-a32eXD0fh_dpyiuJU into data/banana.csv... Done.\n","Downloading 1cBXEmlIAwuOmwPEgd9kbnSvjofGEFIAU into data/book.csv... Done.\n","Downloading 1OZdyx5rXytzXnvTq8S3LZS_Ewl_iyIio into data/fork.csv... Done.\n","Downloading 1585P-SU8G_vNGu78yEJhOpH9-ajxz3fm into data/key.csv... Done.\n","Downloading 1a9KvLtNi3crhi3iqBX93UwKlmPlmxnyi into data/ladder.csv... Done.\n","Downloading 1lnneEBuc2K4papzkui14ZiYN58dMM8VZ into data/pizza.csv... Done.\n","Downloading 1KdDex8cjZc-SNR8NVURsK-OaXtfxEskU into data/stop_sign.csv... Done.\n","Downloading 1fODQI_9LtXyXVk9RNGC7VWZxUlsahxKk into data/tennis_racquet.csv... Done.\n","Downloading 1hbRfukgoLJoQeGtjL820cR89hEqBkzk4 into data/wheel.csv... Done.\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"jPNS_XT5nCIN","colab_type":"text"},"cell_type":"markdown","source":["## **2.2 Reduction, reshape and reorganization of the Dataset:**"]},{"metadata":{"id":"JQuGOMzd-oBm","colab_type":"text"},"cell_type":"markdown","source":["We selected 10 classes from the original datset, which was composed by more than 300 classes. \n","For each drawing, we have a **variable sized** sequence that contains the ordered keypoints of the drawings. By keypoints we mean the important points in a drawing to define its structure (see the next cell for more explanation). We will get these keypoints from the csv file and save them to a folder (train, validation and test) in a \".npy\" format. "]},{"metadata":{"id":"C7wbAlMHZ1kr","colab_type":"code","outputId":"65ce51b5-56b4-4d4d-f074-a0218083dfde","executionInfo":{"status":"ok","timestamp":1544912569071,"user_tz":-60,"elapsed":144391,"user":{"displayName":"Ponç Palau Puigdevall","photoUrl":"https://lh4.googleusercontent.com/-y5VtTnBIWvE/AAAAAAAAAAI/AAAAAAAAApg/KjCyq610BUQ/s64/photo.jpg","userId":"14934553051290892540"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n","step = ['train', 'validation', 'test']\n","\n","dire = r'data/'\n","\n","max_length = 10000 # Maximum number of files (drawings) per class\n","percen=[0.6, 0.3, 0.1] # Percentage of training, validation and testing\n","\n","begin = [0, int(max_length * percen[0]), int(max_length * (percen[0] + percen[1]))]\n","end = [int(max_length * (percen[0])), int(max_length * (percen[0] + percen[1])), max_length]\n","print(begin)\n","print(end)\n","for c in range(0, len(class_name)):\n","  print('Class ' + str(c+1) + ' out of ' + str(len(class_name)))\n","  filename = dire + str(class_name[c]) + '.csv'\n","  \n","  csv = pd.read_csv(filename, sep = ',')\n","  drawing = csv[csv['recognized']==True]\n","  drawings = csv['drawing']\n","  drawings = drawings.values\n","  data = drawings\n","  \n","  for s in range(0, len(step)):\n","    dire_step = str(dire) + str(step[s])\n","    if not os.path.exists(dire_step):\n","      os.makedirs(dire_step)\n","    \n","    for i in range(begin[s], end[s]):\n","      dire_class = str(dire_step) + '/' + str(class_name[c])\n","      if not os.path.exists(dire_class):\n","        os.makedirs(dire_class)\n","        \n","       \n","      x = np.array(json.loads(drawings[i]))\n","      drawing_strokes = []\n","      for elem in x:\n","        mat = np.zeros((2, len(elem[0])))\n","        mat[0, :] = elem[0][:]\n","        mat[1, :] = elem[1][:]\n","        drawing_strokes.append(mat)\n","      aux = np.zeros((2,1)) \n","      for stroke in drawing_strokes:\n","        aux = np.hstack((aux, stroke))\n","      sample_name = class_name[c] + '_' + str(step[s]) + '_' + str(i)\n","      \n","      np.save(os.path.join(dire_class, sample_name), aux[:, 1:])\n","\n","print('Done!')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[0, 6000, 9000]\n","[6000, 9000, 10000]\n","Class 1 out of 10\n","Class 2 out of 10\n","Class 3 out of 10\n","Class 4 out of 10\n","Class 5 out of 10\n","Class 6 out of 10\n","Class 7 out of 10\n","Class 8 out of 10\n","Class 9 out of 10\n","Class 10 out of 10\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"HhoE0veRfHoT","colab_type":"text"},"cell_type":"markdown","source":["## **2.3 Dataset Visualization:**\n","\n","Let us draw the keypoints of a random drawing of an apple. The number next to each keypoint is the order in which this keypoint was drawn. In addition to this plot, a link to a video that makes the drawing in real time is provided."]},{"metadata":{"id":"jh0RVxYHnZbS","colab_type":"code","colab":{}},"cell_type":"code","source":["csv = pd.read_csv('data/apple.csv', sep = ',',engine = 'python')\n","\n","# Header Format\n","#['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word'] \n","\n","drawing = csv[csv['recognized']==True]\n","drawings = csv['drawing']\n","drawings = drawings.values\n","index = 100000 # This is the good sample to show in presentation\n","x = np.array(json.loads(drawings[index]))\n","print(type(x))\n","print(np.shape(x))\n","print(x)\n","img = np.zeros((256, 256))\n","\n","for elem in x:\n","\tfor i in range(0,len(elem[0])-1):\n","\t\t#plt.subplot(211)\n","\t\tplt.plot([elem[0][i], elem[0][i+1]], [-elem[1][i], -elem[1][i+1]], marker = 'o')\t\n","\t\tplt.pause(0.05)\n","\timg[elem[1][:], elem[0][:]] = 255\n","\n","#plt.subplot(212)\n","#plt.imshow(img)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SM8HHsj3Xw9p","colab_type":"text"},"cell_type":"markdown","source":["# **3. Data Loading**\n","\n","In this section we define the data loaders and the auxiliary classes that make possible loading sequences of different lenghts."]},{"metadata":{"id":"9Dbh7DIbgvgc","colab_type":"text"},"cell_type":"markdown","source":["## **3.1  Auxiliary classes to get variable sized sequences**\n","\n","We have the data organized in folders. We have a folder for the training data, a folder for the validation data, and a folder for the test data. For example, for the training data, we have 10 folders, one for each class, each one containing the training samples in the \".npy\" format. This can be seen running the following commands, the first one shows the structure of the training folder and the second one counts the number of files in the data/training/apple directory.\n","\n"]},{"metadata":{"id":"9WeS_kaZMsRR","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls data/training/\n","!ls -l data/training/apple/ | egrep -c '^-'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hnkSCwVmORxG","colab_type":"text"},"cell_type":"markdown","source":["Having this structure, we can take advantage of the PyTorch class [DatasetFolder](https://pytorch.org/docs/0.4.0/torchvision/datasets.html#datasetfolder). This class creates a datasetFolder object that **understands every folder in the root directory as a different class**, and inside every folder of each class it expects to find all the samples of that class. These samples can be in the extension that we want, in our case: \".npy\".  \n","\n","However, we need more things than a DatasetFolder object in order to load batches automatically while training our network, and we need ***even more*** things if our data is variable-sized. \n","\n","Let us go step by step. Once the DatasetFolder object is created, we need an object that **gets the data from that Dataset automatically**, that object is called [DataLoader](https://pytorch.org/docs/0.4.0/data.html#torch.utils.data.DataLoader ). The DataLoader is created taking as argument the DatasetFolder object so it knows where to get the data and how to get it. In addition, and here is where the variable-sized sequences come into play, **we can define a method that preprocess our data before building a batch and pass it as argument!** To do that, we create a class called PadCollate that deals with this problem. \n","\n","This class will take a batch of different length sequences, it will compute the length of all of them, and finally it will zero-pad all the sequences in decreasing length. That is, if we have a batch of 4 sequences with lengths {3, 4, 6, 10}, we will end with a batch of 4 sequences of length 10. We want to preserve the length of the sequence, otherwise, the LSTM won't be able to freeze its hidden layers when the padded zeros are passed as input (it will just take the important part of the input sequences). \n"]},{"metadata":{"id":"6xNq_vmpwdTz","colab_type":"code","colab":{}},"cell_type":"code","source":["def pad_tensor(my_vec, my_pad, my_dim):\n","    \"\"\"\n","    args:\n","        my_vec - tensor to my_pad\n","        my_pad - the size to my_pad to\n","        my_dim - my_dimension to my_pad\n","\n","    return:\n","        a new tensor my_padded to 'my_pad' in my_dimension 'my_dim'\n","    \"\"\"\n","    my_pad_size = list(my_vec.shape)\n","    my_pad_size[my_dim] = my_pad - my_vec.size(my_dim)\n","    return torch.cat([my_vec, torch.zeros(*my_pad_size).double()], dim=my_dim)\n","\n","class PadCollate:\n","\t\"\"\"\n","\ta variant of collate_fn that pads according to the longest sequence in\n","\ta batch of sequences\n","\t\"\"\"\n","\tdef __init__(self, dim=0):\n","\t\t\"\"\"\n","\t\targs:\n","\t\tdim - the dimension to be padded (dimension of time in sequences)\n","\t\t\"\"\"\n","\t\tself.dim = dim\n","\n","\n","\tdef pad_collate(self, batch):\n","\n","\t\t# find longest sequence\n","\t\tlengths = np.flip(np.sort([x[0].shape[self.dim] for x in batch]), axis = 0)\n","\t\tmax_len = max(map(lambda x: x[0].shape[self.dim], batch))\n","\t\tbatch = list(map(lambda x: (pad_tensor(x[0], my_pad=max_len, my_dim=self.dim), x[1]), batch))\n","\t\t# stack all\n","\t\txs = torch.stack(list(map(lambda x: x[0], batch)), dim=1)\n","\t\tys = torch.tensor(list(map(lambda x: x[1], batch)))\n","\t\treturn xs, ys, lengths\n","\n","\tdef __call__(self, batch):\n","\t\treturn self.pad_collate(batch)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HuX4rAijADid","colab_type":"text"},"cell_type":"markdown","source":["## **3.2 Data Loaders definition**"]},{"metadata":{"id":"4oU8VvQjVRhC","colab_type":"text"},"cell_type":"markdown","source":["In the cell below we define our DatasetFolder objects and the corresponding DataLoaders associated to each DatasetFolder. Notice that to handle the variable-sized sequences we must pass as argument a method that pads the sequence with zeros.\n","\n","We also define our batch size (here called **bs**), since the DataLoader needs to know how many samples it has to get for each batch."]},{"metadata":{"id":"JP9g7gv78BUh","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_sample(x):\n","  \"\"\"\n","  We have to tell to the DatasetFolder object HOW TO load a single example so it can form batches with a few samples each one.\n","  \"\"\"\n","  # We load them in format N x 2 in order to stack them in proper order\n","\treturn torch.from_numpy(np.load(x).T).double()\n","\n","# BATCH SIZE\n","bs = 30\n","\n","# DatasetFolder creation and DataLoader creation\n","train_dir = r\"data/train\"\n","val_dir = r\"data/validation\"\n","test_dir = r\"data/test\"\n","\n","train_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 0, collate_fn = PadCollate(dim=0))\n","train_iter = iter(train_loader)\n","\n","valid_dataset = datasets.DatasetFolder(val_dir, extensions = ['.npy'], loader = load_sample)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = bs, shuffle = True, num_workers = 0, collate_fn = PadCollate(dim = 0))\n","valid_iter = iter(valid_loader)\n","\n","test_dataset = datasets.DatasetFolder(test_dir, extensions = ['.npy'], loader = load_sample)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = bs, shuffle = True, num_workers = 0, collate_fn = PadCollate(dim = 0))\n","test_iter = iter(test_loader)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q8pQn318ZHPE","colab_type":"text"},"cell_type":"markdown","source":["To check that the resulting batch has the expected size, we load one single batch."]},{"metadata":{"id":"yuDTFxOvYs6U","colab_type":"code","colab":{}},"cell_type":"code","source":["samples, labels, lengths = valid_iter.next() \n","print(\"The size of the batch is:\")\n","print(samples.size())\n","print(\"The size of the labels is:\")\n","print(labels.size())\n","print(\"The size of the length is:\")\n","print(np.shape(lengths))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TPxaZbBYZmRm","colab_type":"text"},"cell_type":"markdown","source":["# 4. Model definition and training"]},{"metadata":{"id":"8TnHl4EaZr0B","colab_type":"text"},"cell_type":"markdown","source":["Now that we have our dataset structured in folders, our dataloaders that get batches automatically, it is time to build the model! \n","\n","The model that we are going to train is an LSTM (Long-Short Term Memory Network). "]},{"metadata":{"id":"ZicgElYxC7fO","colab_type":"code","colab":{}},"cell_type":"code","source":["class Quick_draw_LSTM(nn.Module):\n","  def __init__(self, lstm_input_size, lstm_units, lstm_hidden_units, batch_size, output_dim):\n","    super(Quick_draw_LSTM, self).__init__()\n","    self.lstm_input_size = lstm_input_size\n","    # Number of stacked lstm that we will have \n","    self.lstm_units = lstm_units\n","    # Number of hidden units of each LSTM\n","    self.lstm_hidden_units = lstm_hidden_units\n","    self.batch_size = batch_size\n","    self.output_dim = output_dim\n","    self.__build_model()\n","  \n","  def __build_model(self):\n","    self.lstm = nn.LSTM(input_size = self.lstm_input_size, hidden_size = self.lstm_hidden_units,num_layers = self.lstm_units)\n","    self.hidden_to_class = nn.Linear(self.lstm_hidden_units, self.output_dim)\n","    \n","  def init_hidden(self):\n","    hidden_a = torch.zeros(self.lstm_units, self.batch_size, self.lstm_hidden_units)\n","    hidden_b = torch.zeros(self.lstm_units, self.batch_size, self.lstm_hidden_units)\n","    return (hidden_a.to(device), hidden_b.to(device))\n","  \n","  def forward(self, X, X_lengths):\n","    # at the beginning of each sequence we must reset the hidden states\n","    self.hidden = self.init_hidden()\n","    seq_len, batch_size, features_size = X.size()\n","    \"\"\"We pack the batch with pack_padded_sequence, this method is useful because the LSTM won't see\n","    the padded values. This function expects as arguments: a tensor of (T x B x *)\n","    \"\"\"\n","    X = nn.utils.rnn.pack_padded_sequence(X, X_lengths)\n","    X, self.hidden = self.lstm(X, self.hidden)\n","    X = nn.utils.rnn.pad_packed_sequence(X)\n","    #print(\"El tipus de X[0] és: \" +str(type(X[0]))) --> torch.Tensor\n","    \n","    X = X[0].contiguous()\n","    X = X.view(-1, X.shape[2])\n","    X = self.hidden_to_class(X)\n","    \n","    #X = F.log_softmax(X, dim=1) #If we use crossEntropy loss it computes the softmax for us!!\n","    X = X.view(seq_len, batch_size, self.output_dim)\n","    return X"],"execution_count":0,"outputs":[]},{"metadata":{"id":"esOsTmQft5fZ","colab_type":"text"},"cell_type":"markdown","source":["# **4. Network Training**"]},{"metadata":{"id":"Qck8TM6sARP5","colab_type":"code","outputId":"0f062559-cbac-4c5d-9d87-f29bcc992e14","executionInfo":{"status":"ok","timestamp":1544912587565,"user_tz":-60,"elapsed":162818,"user":{"displayName":"Ponç Palau Puigdevall","photoUrl":"https://lh4.googleusercontent.com/-y5VtTnBIWvE/AAAAAAAAAAI/AAAAAAAAApg/KjCyq610BUQ/s64/photo.jpg","userId":"14934553051290892540"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["\n","model = Quick_draw_LSTM(2, 3, 512, bs, 10)\n","model.to(device)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Quick_draw_LSTM(\n","  (lstm): LSTM(2, 512, num_layers=3)\n","  (hidden_to_class): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"-g-GrAwhyFcp","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir saved_model5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eKoLV6eSt85B","colab_type":"code","outputId":"7db494ea-11ac-4f70-8823-cf2e953850eb","colab":{"base_uri":"https://localhost:8080/","height":7490}},"cell_type":"code","source":["#In these lists we save the loss and accuracy to plot them afterwards\n","training_loss_list = []\n","validation_loss_list = []\n","training_accuracy_list = []\n","validation_accuracy_list = []\n","\n","# Optimization Hyper-parameters\n","epochs = 150\n","learning_rate = 0.01\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","\n","for epoch in range(epochs):\n","  \n","  running_loss = 0.0\n","  training_accuracy = 0.0\n","  training_total = 0.0\n","  training_correct = 0.0\n","  \n","  for i, data in enumerate(train_loader, 0):\n","    model.train()\n","    # Sample a batch. Recall that we need the lengths for the padding and packing!\n","    inputs, labels, lengths = data\n","   \n","    inputs = inputs.float().to(device)\n","    labels = labels.to(device)\n","    lengths = torch.from_numpy(lengths.copy()).to(device)\n","    \n","    # Zero the gradients because pytorch accumulates gradients\n","    optimizer.zero_grad()\n","    # Forward pass\n","    outputs = model(inputs, lengths)\n","   \n","    # We take only the last output of each sequence! Valid outputs are the outputs computed in the sequence's lengths\n","    valid_outputs = outputs[np.array(lengths-1), np.arange(0, bs), :]\n","    \n","    # Get the indices of the maximum predicted values\n","    _, predicted = torch.max(valid_outputs.data, 1)\n","    # Get the number of correct predictions to compute the accuracy afterwards\n","    training_total = training_total + labels.size(0)\n","    training_correct = training_correct + (predicted == labels).sum().item()\n","    \n","    # Compute the cross entropy loss\n","    loss = criterion(valid_outputs, labels)\n","    \n","    # Backward pass and parameters update\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","    \n","    # Print loss information after 200 batches/epoch\n","    if i % 200 == 199:\n","      training_accuracy = training_correct/training_total\n","      print('[%d, %5d] loss: %.3f - Training_Accuracy: %.3f' % (epoch + 1, i + 1, running_loss / 200, training_accuracy))\n","      training_loss_list.append(running_loss)\n","      training_accuracy_list.append(training_accuracy)\n","      running_loss = 0.0\n","      training_total = 0.0\n","      training_correct = 0.0 \n","      \n","  with torch.no_grad():\n","\n","    validation_correct = 0.0\n","    validation_accuracy = 0.0\n","    validation_total = 0.0\n","    running_validation_loss = 0.0\n","    \n","    model.eval()\n","    \n","    for j, valid_data in enumerate(valid_loader, 0):\n","      validation_inputs, validation_labels, validation_lengths = valid_data\n","      \n","      validation_inputs = validation_inputs.float().to(device)\n","      validation_labels = validation_labels.to(device)\n","      validation_lengths = torch.from_numpy(validation_lengths.copy()).to(device)\n","      validation_outputs = model(validation_inputs, validation_lengths)\n","\n","      validation_valid_outputs = validation_outputs[np.array(validation_lengths-1), np.arange(0, bs), :]\n","\n","      valid_loss = criterion(validation_valid_outputs, validation_labels)\n","      _, val_predicted = torch.max(validation_valid_outputs.data, 1)\n","      validation_total = validation_labels.size(0) + validation_total\n","      validation_correct = (val_predicted == validation_labels).sum().item() + validation_correct\n","      running_validation_loss += valid_loss.item()\n","\n","    validation_accuracy = validation_correct/validation_total\n","    print('[%d, %5d] Validation loss: %.3f - Validation_Accuracy: %.3f' %(epoch + 1, i + 1, running_validation_loss/(len(valid_loader)), validation_accuracy))\n","    validation_loss_list.append(running_validation_loss)\n","    validation_accuracy_list.append(validation_accuracy)\n","    val_correct = 0.0\n","    running_validation_loss = 0.0\n","  #Save the model for early-stopping\n","  torch.save(model.state_dict(), os.path.join('saved_model5', str(epoch)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,   200] loss: 2.229 - Training_Accuracy: 0.161\n","[1,   400] loss: 2.126 - Training_Accuracy: 0.194\n","[1,   600] loss: 2.040 - Training_Accuracy: 0.225\n","[1,   800] loss: 1.959 - Training_Accuracy: 0.271\n","[1,  1000] loss: 1.897 - Training_Accuracy: 0.296\n","[1,  1200] loss: 1.793 - Training_Accuracy: 0.337\n","[1,  1400] loss: 1.764 - Training_Accuracy: 0.350\n","[1,  1600] loss: 1.781 - Training_Accuracy: 0.357\n","[1,  1800] loss: 2.015 - Training_Accuracy: 0.268\n","[1,  2000] loss: 1.847 - Training_Accuracy: 0.331\n","[1,  2000] Validation loss: 1.754 - Validation_Accuracy: 0.357\n","[2,   200] loss: 1.966 - Training_Accuracy: 0.284\n","[2,   400] loss: 1.895 - Training_Accuracy: 0.299\n","[2,   600] loss: 1.788 - Training_Accuracy: 0.345\n","[2,   800] loss: 1.716 - Training_Accuracy: 0.380\n","[2,  1000] loss: 1.656 - Training_Accuracy: 0.411\n","[2,  1200] loss: 1.551 - Training_Accuracy: 0.453\n","[2,  1400] loss: 1.467 - Training_Accuracy: 0.482\n","[2,  1600] loss: 1.434 - Training_Accuracy: 0.492\n","[2,  1800] loss: 1.390 - Training_Accuracy: 0.505\n","[2,  2000] loss: 1.340 - Training_Accuracy: 0.525\n","[2,  2000] Validation loss: 1.325 - Validation_Accuracy: 0.535\n","[3,   200] loss: 1.310 - Training_Accuracy: 0.544\n","[3,   400] loss: 1.299 - Training_Accuracy: 0.546\n","[3,   600] loss: 1.208 - Training_Accuracy: 0.575\n","[3,   800] loss: 1.185 - Training_Accuracy: 0.582\n","[3,  1000] loss: 1.152 - Training_Accuracy: 0.598\n","[3,  1200] loss: 1.144 - Training_Accuracy: 0.605\n","[3,  1400] loss: 1.137 - Training_Accuracy: 0.612\n","[3,  1600] loss: 1.088 - Training_Accuracy: 0.621\n","[3,  1800] loss: 1.056 - Training_Accuracy: 0.638\n","[3,  2000] loss: 1.052 - Training_Accuracy: 0.635\n","[3,  2000] Validation loss: 1.081 - Validation_Accuracy: 0.628\n","[4,   200] loss: 1.034 - Training_Accuracy: 0.640\n","[4,   400] loss: 0.973 - Training_Accuracy: 0.666\n","[4,   600] loss: 0.960 - Training_Accuracy: 0.671\n","[4,   800] loss: 0.934 - Training_Accuracy: 0.675\n","[4,  1000] loss: 0.926 - Training_Accuracy: 0.682\n","[4,  1200] loss: 0.908 - Training_Accuracy: 0.683\n","[4,  1400] loss: 0.897 - Training_Accuracy: 0.691\n","[4,  1600] loss: 0.862 - Training_Accuracy: 0.708\n","[4,  1800] loss: 0.875 - Training_Accuracy: 0.696\n","[4,  2000] loss: 0.848 - Training_Accuracy: 0.704\n","[4,  2000] Validation loss: 0.831 - Validation_Accuracy: 0.715\n","[5,   200] loss: 0.833 - Training_Accuracy: 0.714\n","[5,   400] loss: 0.829 - Training_Accuracy: 0.717\n","[5,   600] loss: 0.800 - Training_Accuracy: 0.721\n","[5,   800] loss: 0.810 - Training_Accuracy: 0.727\n","[5,  1000] loss: 0.760 - Training_Accuracy: 0.744\n","[5,  1200] loss: 0.772 - Training_Accuracy: 0.730\n","[5,  1400] loss: 0.792 - Training_Accuracy: 0.727\n","[5,  1600] loss: 0.792 - Training_Accuracy: 0.724\n","[5,  1800] loss: 0.769 - Training_Accuracy: 0.732\n","[5,  2000] loss: 0.751 - Training_Accuracy: 0.742\n","[5,  2000] Validation loss: 0.760 - Validation_Accuracy: 0.740\n","[6,   200] loss: 0.769 - Training_Accuracy: 0.733\n","[6,   400] loss: 0.734 - Training_Accuracy: 0.744\n","[6,   600] loss: 0.730 - Training_Accuracy: 0.751\n","[6,   800] loss: 0.744 - Training_Accuracy: 0.740\n","[6,  1000] loss: 0.729 - Training_Accuracy: 0.757\n","[6,  1200] loss: 0.752 - Training_Accuracy: 0.741\n","[6,  1400] loss: 0.684 - Training_Accuracy: 0.762\n","[6,  1600] loss: 0.718 - Training_Accuracy: 0.750\n","[6,  1800] loss: 0.683 - Training_Accuracy: 0.758\n","[6,  2000] loss: 0.714 - Training_Accuracy: 0.754\n","[6,  2000] Validation loss: 0.697 - Validation_Accuracy: 0.761\n","[7,   200] loss: 0.703 - Training_Accuracy: 0.757\n","[7,   400] loss: 0.659 - Training_Accuracy: 0.775\n","[7,   600] loss: 0.692 - Training_Accuracy: 0.762\n","[7,   800] loss: 0.679 - Training_Accuracy: 0.768\n","[7,  1000] loss: 0.687 - Training_Accuracy: 0.759\n","[7,  1200] loss: 0.676 - Training_Accuracy: 0.767\n","[7,  1400] loss: 0.675 - Training_Accuracy: 0.768\n","[7,  1600] loss: 0.685 - Training_Accuracy: 0.760\n","[7,  1800] loss: 0.675 - Training_Accuracy: 0.763\n","[7,  2000] loss: 0.656 - Training_Accuracy: 0.776\n","[7,  2000] Validation loss: 0.734 - Validation_Accuracy: 0.744\n","[8,   200] loss: 0.660 - Training_Accuracy: 0.776\n","[8,   400] loss: 0.638 - Training_Accuracy: 0.778\n","[8,   600] loss: 0.656 - Training_Accuracy: 0.771\n","[8,   800] loss: 0.644 - Training_Accuracy: 0.778\n","[8,  1000] loss: 0.626 - Training_Accuracy: 0.785\n","[8,  1200] loss: 0.635 - Training_Accuracy: 0.780\n","[8,  1400] loss: 0.625 - Training_Accuracy: 0.784\n","[8,  1600] loss: 0.597 - Training_Accuracy: 0.791\n","[8,  1800] loss: 0.658 - Training_Accuracy: 0.771\n","[8,  2000] loss: 0.645 - Training_Accuracy: 0.778\n","[8,  2000] Validation loss: 0.619 - Validation_Accuracy: 0.786\n","[9,   200] loss: 0.642 - Training_Accuracy: 0.776\n","[9,   400] loss: 0.626 - Training_Accuracy: 0.785\n","[9,   600] loss: 0.622 - Training_Accuracy: 0.783\n","[9,   800] loss: 0.626 - Training_Accuracy: 0.784\n","[9,  1000] loss: 0.621 - Training_Accuracy: 0.786\n","[9,  1200] loss: 0.622 - Training_Accuracy: 0.785\n","[9,  1400] loss: 0.624 - Training_Accuracy: 0.784\n","[9,  1600] loss: 0.633 - Training_Accuracy: 0.776\n","[9,  1800] loss: 0.589 - Training_Accuracy: 0.794\n","[9,  2000] loss: 0.571 - Training_Accuracy: 0.801\n","[9,  2000] Validation loss: 0.699 - Validation_Accuracy: 0.758\n","[10,   200] loss: 0.610 - Training_Accuracy: 0.786\n","[10,   400] loss: 0.578 - Training_Accuracy: 0.802\n","[10,   600] loss: 0.578 - Training_Accuracy: 0.791\n","[10,   800] loss: 0.579 - Training_Accuracy: 0.793\n","[10,  1000] loss: 0.607 - Training_Accuracy: 0.792\n","[10,  1200] loss: 0.571 - Training_Accuracy: 0.803\n","[10,  1400] loss: 0.620 - Training_Accuracy: 0.784\n","[10,  1600] loss: 0.578 - Training_Accuracy: 0.801\n","[10,  1800] loss: 0.578 - Training_Accuracy: 0.803\n","[10,  2000] loss: 0.584 - Training_Accuracy: 0.796\n","[10,  2000] Validation loss: 0.665 - Validation_Accuracy: 0.769\n","[11,   200] loss: 0.561 - Training_Accuracy: 0.796\n","[11,   400] loss: 0.552 - Training_Accuracy: 0.810\n","[11,   600] loss: 0.539 - Training_Accuracy: 0.811\n","[11,   800] loss: 0.544 - Training_Accuracy: 0.809\n","[11,  1000] loss: 0.582 - Training_Accuracy: 0.800\n","[11,  1200] loss: 0.540 - Training_Accuracy: 0.814\n","[11,  1400] loss: 0.590 - Training_Accuracy: 0.790\n","[11,  1600] loss: 0.601 - Training_Accuracy: 0.795\n","[11,  1800] loss: 0.585 - Training_Accuracy: 0.797\n","[11,  2000] loss: 0.566 - Training_Accuracy: 0.806\n","[11,  2000] Validation loss: 0.625 - Validation_Accuracy: 0.785\n","[12,   200] loss: 0.583 - Training_Accuracy: 0.796\n","[12,   400] loss: 0.583 - Training_Accuracy: 0.803\n","[12,   600] loss: 0.534 - Training_Accuracy: 0.816\n","[12,   800] loss: 0.575 - Training_Accuracy: 0.800\n","[12,  1000] loss: 0.542 - Training_Accuracy: 0.814\n","[12,  1200] loss: 0.577 - Training_Accuracy: 0.804\n","[12,  1400] loss: 0.561 - Training_Accuracy: 0.806\n","[12,  1600] loss: 0.551 - Training_Accuracy: 0.810\n","[12,  1800] loss: 0.554 - Training_Accuracy: 0.805\n","[12,  2000] loss: 0.554 - Training_Accuracy: 0.806\n","[12,  2000] Validation loss: 0.570 - Validation_Accuracy: 0.801\n","[13,   200] loss: 0.550 - Training_Accuracy: 0.808\n","[13,   400] loss: 0.531 - Training_Accuracy: 0.815\n","[13,   600] loss: 0.543 - Training_Accuracy: 0.805\n","[13,   800] loss: 0.533 - Training_Accuracy: 0.813\n","[13,  1000] loss: 0.544 - Training_Accuracy: 0.808\n","[13,  1200] loss: 0.547 - Training_Accuracy: 0.809\n","[13,  1400] loss: 0.551 - Training_Accuracy: 0.812\n","[13,  1600] loss: 0.516 - Training_Accuracy: 0.820\n","[13,  1800] loss: 0.522 - Training_Accuracy: 0.818\n","[13,  2000] loss: 0.544 - Training_Accuracy: 0.809\n","[13,  2000] Validation loss: 0.545 - Validation_Accuracy: 0.813\n","[14,   200] loss: 0.547 - Training_Accuracy: 0.809\n","[14,   400] loss: 0.559 - Training_Accuracy: 0.803\n","[14,   600] loss: 0.538 - Training_Accuracy: 0.812\n","[14,   800] loss: 0.531 - Training_Accuracy: 0.814\n","[14,  1000] loss: 0.535 - Training_Accuracy: 0.813\n","[14,  1200] loss: 0.528 - Training_Accuracy: 0.814\n","[14,  1400] loss: 0.527 - Training_Accuracy: 0.815\n","[14,  1600] loss: 0.536 - Training_Accuracy: 0.813\n","[14,  1800] loss: 0.531 - Training_Accuracy: 0.815\n","[14,  2000] loss: 0.538 - Training_Accuracy: 0.813\n","[14,  2000] Validation loss: 0.614 - Validation_Accuracy: 0.784\n","[15,   200] loss: 0.532 - Training_Accuracy: 0.812\n","[15,   400] loss: 0.534 - Training_Accuracy: 0.818\n","[15,   600] loss: 0.522 - Training_Accuracy: 0.816\n","[15,   800] loss: 0.525 - Training_Accuracy: 0.806\n","[15,  1000] loss: 0.535 - Training_Accuracy: 0.820\n","[15,  1200] loss: 0.513 - Training_Accuracy: 0.821\n","[15,  1400] loss: 0.511 - Training_Accuracy: 0.817\n","[15,  1600] loss: 0.526 - Training_Accuracy: 0.813\n","[15,  1800] loss: 0.529 - Training_Accuracy: 0.818\n","[15,  2000] loss: 0.515 - Training_Accuracy: 0.817\n","[15,  2000] Validation loss: 0.557 - Validation_Accuracy: 0.806\n","[16,   200] loss: 0.494 - Training_Accuracy: 0.824\n","[16,   400] loss: 0.489 - Training_Accuracy: 0.825\n","[16,   600] loss: 0.513 - Training_Accuracy: 0.819\n","[16,   800] loss: 0.507 - Training_Accuracy: 0.826\n","[16,  1000] loss: 0.474 - Training_Accuracy: 0.834\n","[16,  1200] loss: 0.490 - Training_Accuracy: 0.835\n","[16,  1400] loss: 0.512 - Training_Accuracy: 0.820\n","[16,  1600] loss: 0.504 - Training_Accuracy: 0.824\n","[16,  1800] loss: 0.493 - Training_Accuracy: 0.830\n","[16,  2000] loss: 0.501 - Training_Accuracy: 0.825\n","[16,  2000] Validation loss: 0.585 - Validation_Accuracy: 0.796\n","[17,   200] loss: 0.512 - Training_Accuracy: 0.818\n","[17,   400] loss: 0.506 - Training_Accuracy: 0.823\n","[17,   600] loss: 0.504 - Training_Accuracy: 0.826\n","[17,   800] loss: 0.483 - Training_Accuracy: 0.830\n","[17,  1000] loss: 0.498 - Training_Accuracy: 0.831\n","[17,  1200] loss: 0.498 - Training_Accuracy: 0.824\n","[17,  1400] loss: 0.489 - Training_Accuracy: 0.827\n","[17,  1600] loss: 0.516 - Training_Accuracy: 0.818\n","[17,  1800] loss: 0.488 - Training_Accuracy: 0.829\n","[17,  2000] loss: 0.499 - Training_Accuracy: 0.826\n","[17,  2000] Validation loss: 0.506 - Validation_Accuracy: 0.823\n","[18,   200] loss: 0.473 - Training_Accuracy: 0.840\n","[18,   400] loss: 0.481 - Training_Accuracy: 0.831\n","[18,   600] loss: 0.512 - Training_Accuracy: 0.821\n","[18,   800] loss: 0.483 - Training_Accuracy: 0.828\n","[18,  1000] loss: 0.478 - Training_Accuracy: 0.832\n","[18,  1200] loss: 0.492 - Training_Accuracy: 0.827\n","[18,  1400] loss: 0.484 - Training_Accuracy: 0.828\n","[18,  1600] loss: 0.486 - Training_Accuracy: 0.829\n","[18,  1800] loss: 0.495 - Training_Accuracy: 0.820\n","[18,  2000] loss: 0.498 - Training_Accuracy: 0.825\n","[18,  2000] Validation loss: 0.561 - Validation_Accuracy: 0.804\n","[19,   200] loss: 0.473 - Training_Accuracy: 0.830\n","[19,   400] loss: 0.478 - Training_Accuracy: 0.832\n","[19,   600] loss: 0.486 - Training_Accuracy: 0.834\n","[19,   800] loss: 0.469 - Training_Accuracy: 0.835\n","[19,  1000] loss: 0.485 - Training_Accuracy: 0.836\n","[19,  1200] loss: 0.470 - Training_Accuracy: 0.830\n","[19,  1400] loss: 0.477 - Training_Accuracy: 0.834\n","[19,  1600] loss: 0.492 - Training_Accuracy: 0.826\n","[19,  1800] loss: 0.480 - Training_Accuracy: 0.835\n","[19,  2000] loss: 0.496 - Training_Accuracy: 0.829\n","[19,  2000] Validation loss: 0.524 - Validation_Accuracy: 0.821\n","[20,   200] loss: 0.492 - Training_Accuracy: 0.833\n","[20,   400] loss: 0.514 - Training_Accuracy: 0.813\n","[20,   600] loss: 0.458 - Training_Accuracy: 0.836\n","[20,   800] loss: 0.503 - Training_Accuracy: 0.823\n","[20,  1000] loss: 0.455 - Training_Accuracy: 0.842\n","[20,  1200] loss: 0.473 - Training_Accuracy: 0.833\n","[20,  1400] loss: 0.503 - Training_Accuracy: 0.819\n","[20,  1600] loss: 0.475 - Training_Accuracy: 0.831\n","[20,  1800] loss: 0.472 - Training_Accuracy: 0.840\n","[20,  2000] loss: 0.473 - Training_Accuracy: 0.826\n","[20,  2000] Validation loss: 0.577 - Validation_Accuracy: 0.799\n","[21,   200] loss: 0.477 - Training_Accuracy: 0.836\n","[21,   400] loss: 0.476 - Training_Accuracy: 0.829\n","[21,   600] loss: 0.489 - Training_Accuracy: 0.832\n","[21,   800] loss: 0.463 - Training_Accuracy: 0.842\n","[21,  1000] loss: 0.487 - Training_Accuracy: 0.828\n","[21,  1200] loss: 0.457 - Training_Accuracy: 0.840\n","[21,  1400] loss: 0.475 - Training_Accuracy: 0.836\n","[21,  1600] loss: 0.458 - Training_Accuracy: 0.835\n","[21,  1800] loss: 0.469 - Training_Accuracy: 0.835\n","[21,  2000] loss: 0.499 - Training_Accuracy: 0.823\n","[21,  2000] Validation loss: 0.532 - Validation_Accuracy: 0.815\n","[22,   200] loss: 0.479 - Training_Accuracy: 0.832\n","[22,   400] loss: 0.450 - Training_Accuracy: 0.834\n","[22,   600] loss: 0.468 - Training_Accuracy: 0.838\n","[22,   800] loss: 0.436 - Training_Accuracy: 0.843\n","[22,  1000] loss: 0.454 - Training_Accuracy: 0.834\n","[22,  1200] loss: 0.436 - Training_Accuracy: 0.842\n","[22,  1400] loss: 0.458 - Training_Accuracy: 0.837\n","[22,  1600] loss: 0.483 - Training_Accuracy: 0.827\n","[22,  1800] loss: 0.478 - Training_Accuracy: 0.830\n","[22,  2000] loss: 0.485 - Training_Accuracy: 0.829\n","[22,  2000] Validation loss: 0.515 - Validation_Accuracy: 0.822\n","[23,   200] loss: 0.470 - Training_Accuracy: 0.838\n","[23,   400] loss: 0.460 - Training_Accuracy: 0.835\n","[23,   600] loss: 0.465 - Training_Accuracy: 0.832\n","[23,   800] loss: 0.459 - Training_Accuracy: 0.836\n","[23,  1000] loss: 0.442 - Training_Accuracy: 0.844\n","[23,  1200] loss: 0.478 - Training_Accuracy: 0.832\n","[23,  1400] loss: 0.464 - Training_Accuracy: 0.834\n","[23,  1600] loss: 0.444 - Training_Accuracy: 0.841\n","[23,  1800] loss: 0.454 - Training_Accuracy: 0.841\n","[23,  2000] loss: 0.486 - Training_Accuracy: 0.824\n","[23,  2000] Validation loss: 0.515 - Validation_Accuracy: 0.819\n","[24,   200] loss: 0.439 - Training_Accuracy: 0.838\n","[24,   400] loss: 0.447 - Training_Accuracy: 0.841\n","[24,   600] loss: 0.471 - Training_Accuracy: 0.839\n","[24,   800] loss: 0.449 - Training_Accuracy: 0.840\n","[24,  1000] loss: 0.426 - Training_Accuracy: 0.848\n","[24,  1200] loss: 0.434 - Training_Accuracy: 0.843\n","[24,  1400] loss: 0.430 - Training_Accuracy: 0.844\n","[24,  1600] loss: 0.454 - Training_Accuracy: 0.840\n","[24,  1800] loss: 0.457 - Training_Accuracy: 0.837\n","[24,  2000] loss: 0.458 - Training_Accuracy: 0.836\n","[24,  2000] Validation loss: 0.514 - Validation_Accuracy: 0.819\n","[25,   200] loss: 0.432 - Training_Accuracy: 0.846\n","[25,   400] loss: 0.419 - Training_Accuracy: 0.848\n","[25,   600] loss: 0.435 - Training_Accuracy: 0.847\n","[25,   800] loss: 0.423 - Training_Accuracy: 0.853\n","[25,  1000] loss: 0.425 - Training_Accuracy: 0.844\n","[25,  1200] loss: 0.424 - Training_Accuracy: 0.851\n","[25,  1400] loss: 0.438 - Training_Accuracy: 0.851\n","[25,  1600] loss: 0.422 - Training_Accuracy: 0.846\n","[25,  1800] loss: 0.422 - Training_Accuracy: 0.847\n","[25,  2000] loss: 0.443 - Training_Accuracy: 0.840\n","[25,  2000] Validation loss: 0.468 - Validation_Accuracy: 0.835\n","[26,   200] loss: 0.416 - Training_Accuracy: 0.851\n","[26,   400] loss: 0.420 - Training_Accuracy: 0.847\n","[26,   600] loss: 0.401 - Training_Accuracy: 0.855\n","[26,   800] loss: 0.428 - Training_Accuracy: 0.847\n","[26,  1000] loss: 0.435 - Training_Accuracy: 0.844\n","[26,  1200] loss: 0.420 - Training_Accuracy: 0.846\n","[26,  1400] loss: 0.404 - Training_Accuracy: 0.855\n","[26,  1600] loss: 0.441 - Training_Accuracy: 0.842\n","[26,  1800] loss: 0.442 - Training_Accuracy: 0.844\n","[26,  2000] loss: 0.438 - Training_Accuracy: 0.845\n","[26,  2000] Validation loss: 0.484 - Validation_Accuracy: 0.832\n","[27,   200] loss: 0.383 - Training_Accuracy: 0.859\n","[27,   400] loss: 0.419 - Training_Accuracy: 0.854\n","[27,   600] loss: 0.428 - Training_Accuracy: 0.850\n","[27,   800] loss: 0.451 - Training_Accuracy: 0.834\n","[27,  1000] loss: 0.429 - Training_Accuracy: 0.847\n","[27,  1200] loss: 0.471 - Training_Accuracy: 0.834\n","[27,  1400] loss: 0.428 - Training_Accuracy: 0.846\n","[27,  1600] loss: 0.444 - Training_Accuracy: 0.845\n","[27,  1800] loss: 0.408 - Training_Accuracy: 0.851\n","[27,  2000] loss: 0.400 - Training_Accuracy: 0.857\n","[27,  2000] Validation loss: 0.484 - Validation_Accuracy: 0.830\n","[28,   200] loss: 0.408 - Training_Accuracy: 0.858\n","[28,   400] loss: 0.411 - Training_Accuracy: 0.852\n","[28,   600] loss: 0.399 - Training_Accuracy: 0.854\n","[28,   800] loss: 0.413 - Training_Accuracy: 0.851\n","[28,  1000] loss: 0.406 - Training_Accuracy: 0.853\n","[28,  1200] loss: 0.394 - Training_Accuracy: 0.856\n","[28,  1400] loss: 0.430 - Training_Accuracy: 0.846\n","[28,  1600] loss: 0.417 - Training_Accuracy: 0.846\n","[28,  1800] loss: 0.417 - Training_Accuracy: 0.853\n","[28,  2000] loss: 0.412 - Training_Accuracy: 0.852\n","[28,  2000] Validation loss: 0.466 - Validation_Accuracy: 0.840\n","[29,   200] loss: 0.415 - Training_Accuracy: 0.848\n","[29,   400] loss: 0.398 - Training_Accuracy: 0.860\n","[29,   600] loss: 0.417 - Training_Accuracy: 0.849\n","[29,   800] loss: 0.399 - Training_Accuracy: 0.857\n","[29,  1000] loss: 0.409 - Training_Accuracy: 0.855\n","[29,  1200] loss: 0.383 - Training_Accuracy: 0.863\n","[29,  1400] loss: 0.408 - Training_Accuracy: 0.855\n","[29,  1600] loss: 0.417 - Training_Accuracy: 0.848\n","[29,  1800] loss: 0.431 - Training_Accuracy: 0.846\n","[29,  2000] loss: 0.414 - Training_Accuracy: 0.849\n","[29,  2000] Validation loss: 0.466 - Validation_Accuracy: 0.835\n","[30,   200] loss: 0.389 - Training_Accuracy: 0.862\n","[30,   400] loss: 0.402 - Training_Accuracy: 0.857\n","[30,   600] loss: 0.400 - Training_Accuracy: 0.857\n","[30,   800] loss: 0.411 - Training_Accuracy: 0.852\n","[30,  1000] loss: 0.446 - Training_Accuracy: 0.843\n","[30,  1200] loss: 0.399 - Training_Accuracy: 0.852\n","[30,  1400] loss: 0.413 - Training_Accuracy: 0.848\n","[30,  1600] loss: 0.431 - Training_Accuracy: 0.846\n","[30,  1800] loss: 0.425 - Training_Accuracy: 0.851\n","[30,  2000] loss: 0.441 - Training_Accuracy: 0.843\n","[30,  2000] Validation loss: 0.475 - Validation_Accuracy: 0.831\n","[31,   200] loss: 0.411 - Training_Accuracy: 0.851\n","[31,   400] loss: 0.406 - Training_Accuracy: 0.854\n","[31,   600] loss: 0.398 - Training_Accuracy: 0.857\n","[31,   800] loss: 0.397 - Training_Accuracy: 0.857\n","[31,  1000] loss: 0.400 - Training_Accuracy: 0.858\n","[31,  1200] loss: 0.417 - Training_Accuracy: 0.852\n","[31,  1400] loss: 0.406 - Training_Accuracy: 0.855\n","[31,  1600] loss: 0.393 - Training_Accuracy: 0.857\n","[31,  1800] loss: 0.419 - Training_Accuracy: 0.853\n","[31,  2000] loss: 0.388 - Training_Accuracy: 0.861\n","[31,  2000] Validation loss: 0.499 - Validation_Accuracy: 0.826\n","[32,   200] loss: 0.400 - Training_Accuracy: 0.853\n","[32,   400] loss: 0.417 - Training_Accuracy: 0.848\n","[32,   600] loss: 0.393 - Training_Accuracy: 0.859\n","[32,   800] loss: 0.404 - Training_Accuracy: 0.859\n","[32,  1000] loss: 0.411 - Training_Accuracy: 0.854\n","[32,  1200] loss: 0.409 - Training_Accuracy: 0.854\n","[32,  1400] loss: 0.377 - Training_Accuracy: 0.868\n","[32,  1600] loss: 0.407 - Training_Accuracy: 0.849\n","[32,  1800] loss: 0.374 - Training_Accuracy: 0.867\n","[32,  2000] loss: 0.395 - Training_Accuracy: 0.859\n","[32,  2000] Validation loss: 0.443 - Validation_Accuracy: 0.844\n","[33,   200] loss: 0.382 - Training_Accuracy: 0.862\n","[33,   400] loss: 0.399 - Training_Accuracy: 0.852\n","[33,   600] loss: 0.397 - Training_Accuracy: 0.859\n","[33,   800] loss: 0.397 - Training_Accuracy: 0.853\n","[33,  1000] loss: 0.393 - Training_Accuracy: 0.860\n","[33,  1200] loss: 0.379 - Training_Accuracy: 0.860\n","[33,  1400] loss: 0.392 - Training_Accuracy: 0.856\n","[33,  1600] loss: 0.376 - Training_Accuracy: 0.860\n","[33,  1800] loss: 0.383 - Training_Accuracy: 0.863\n","[33,  2000] loss: 0.411 - Training_Accuracy: 0.850\n","[33,  2000] Validation loss: 0.448 - Validation_Accuracy: 0.842\n","[34,   200] loss: 0.369 - Training_Accuracy: 0.865\n","[34,   400] loss: 0.390 - Training_Accuracy: 0.859\n","[34,   600] loss: 0.421 - Training_Accuracy: 0.851\n","[34,   800] loss: 0.417 - Training_Accuracy: 0.849\n","[34,  1000] loss: 0.410 - Training_Accuracy: 0.849\n","[34,  1200] loss: 0.417 - Training_Accuracy: 0.855\n","[34,  1400] loss: 0.401 - Training_Accuracy: 0.858\n","[34,  1600] loss: 0.392 - Training_Accuracy: 0.861\n","[34,  1800] loss: 0.411 - Training_Accuracy: 0.854\n","[34,  2000] loss: 0.408 - Training_Accuracy: 0.850\n","[34,  2000] Validation loss: 0.525 - Validation_Accuracy: 0.817\n","[35,   200] loss: 0.421 - Training_Accuracy: 0.848\n","[35,   400] loss: 0.395 - Training_Accuracy: 0.852\n","[35,   600] loss: 0.380 - Training_Accuracy: 0.860\n","[35,   800] loss: 0.403 - Training_Accuracy: 0.853\n","[35,  1000] loss: 0.395 - Training_Accuracy: 0.853\n","[35,  1200] loss: 0.387 - Training_Accuracy: 0.865\n","[35,  1400] loss: 0.389 - Training_Accuracy: 0.861\n","[35,  1600] loss: 0.375 - Training_Accuracy: 0.864\n","[35,  1800] loss: 0.409 - Training_Accuracy: 0.852\n","[35,  2000] loss: 0.421 - Training_Accuracy: 0.840\n","[35,  2000] Validation loss: 0.480 - Validation_Accuracy: 0.835\n","[36,   200] loss: 0.409 - Training_Accuracy: 0.850\n","[36,   400] loss: 0.414 - Training_Accuracy: 0.850\n","[36,   600] loss: 0.397 - Training_Accuracy: 0.853\n","[36,   800] loss: 0.410 - Training_Accuracy: 0.850\n","[36,  1000] loss: 0.376 - Training_Accuracy: 0.864\n","[36,  1200] loss: 0.401 - Training_Accuracy: 0.856\n","[36,  1400] loss: 0.401 - Training_Accuracy: 0.858\n","[36,  1600] loss: 0.361 - Training_Accuracy: 0.867\n","[36,  1800] loss: 0.377 - Training_Accuracy: 0.859\n","[36,  2000] loss: 0.389 - Training_Accuracy: 0.858\n","[36,  2000] Validation loss: 0.437 - Validation_Accuracy: 0.844\n","[37,   200] loss: 0.365 - Training_Accuracy: 0.868\n","[37,   400] loss: 0.393 - Training_Accuracy: 0.858\n","[37,   600] loss: 0.372 - Training_Accuracy: 0.868\n","[37,   800] loss: 0.361 - Training_Accuracy: 0.869\n","[37,  1000] loss: 0.394 - Training_Accuracy: 0.859\n","[37,  1200] loss: 0.376 - Training_Accuracy: 0.866\n","[37,  1400] loss: 0.394 - Training_Accuracy: 0.854\n","[37,  1600] loss: 0.361 - Training_Accuracy: 0.865\n","[37,  1800] loss: 0.379 - Training_Accuracy: 0.860\n","[37,  2000] loss: 0.369 - Training_Accuracy: 0.862\n","[37,  2000] Validation loss: 0.429 - Validation_Accuracy: 0.850\n","[38,   200] loss: 0.365 - Training_Accuracy: 0.864\n","[38,   400] loss: 0.357 - Training_Accuracy: 0.873\n","[38,   600] loss: 0.352 - Training_Accuracy: 0.873\n","[38,   800] loss: 0.378 - Training_Accuracy: 0.862\n"],"name":"stdout"}]},{"metadata":{"id":"WTbBCBvsIw3y","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","training_examples = 6e5\n","plot_every = 200 # We plot loss information after 200 forward passes\n","training_loss_np = np.asarray(training_loss_list)\n","#validation_loss_np = np.asarray(validation_loss_list)\n","\n","x_axis = np.arange(0, len(training_loss_list))\n","plt.plot(x_axis * bs * plot_every / training_examples, training_loss_np)\n","#plt.plot(x_axis * bs * plot_every / training_ex, validation_loss_np)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fY5Wc9h6gDoG","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","files.download('example.txt') "],"execution_count":0,"outputs":[]},{"metadata":{"id":"VYGMvzBa1mky","colab_type":"text"},"cell_type":"markdown","source":["# **5. Network Testing**"]},{"metadata":{"id":"GuiC_DNl1q_n","colab_type":"code","colab":{}},"cell_type":"code","source":["running_test_loss=0.0\n","test_total=0.0\n","test_correct=0.0\n","  \n","for i, test_data in enumerate(test_loader,0): \n","  test_inputs, test_labels, test_lengths = test_data\n","  \n","  test_inputs = test_inputs.float().to(device)\n","  test_labels = test_labels.to(device)\n","  test_lengths = torch.from_numpy(test_lengths.copy()).to(device)\n","  test_outputs = model(test_inputs, test_lengths)\n","  test_valid_outputs = validation_outputs[np.array(test_lengths-1), np.arange(0, bs), :]\n","  \n","  test_loss = criterion(test_valid_outputs, test_labels)\n","  running_test_loss += test_loss.item()\n","  \n","  _,predicted = torch.max(test_outputs.data,1)\n","  test_total = test_total+test_labels.size(0)\n","  test_correct = test_correct + (predicted == test_labels).sum().item()        \n","  \n","test_accuracy = test_correct/test_total\n","  \n","print('Test Loss: %.3f - Test Accuracy: %.3f' %\n","         (running_test_loss/len(test_loader), test_accuracy))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"djkk2I7SnB21","colab_type":"text"},"cell_type":"markdown","source":["Let's evaluate the model on the test data:\n","\n","While our densely-connected network we had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we decreased our error rate by 68% (relative)."]},{"metadata":{"id":"PXPvUyOZpj8K","colab_type":"code","colab":{}},"cell_type":"code","source":["# ACCURACY OF THE NETWORK\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z_hhvSIRpzvp","colab_type":"code","colab":{}},"cell_type":"code","source":["#WHICH CLASSESS PERFORMED BETTER\n","class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for i in range(4):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","\n","\n","for i in range(10):\n","    print('Accuracy of %5s : %2d %%' % (\n","        classes[i], 100 * class_correct[i] / class_total[i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oNgsUfJ869qT","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}